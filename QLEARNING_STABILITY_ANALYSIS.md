# Q-Learningç¨³å®šæ€§é—®é¢˜åˆ†æä¸æ”¹è¿›æ–¹æ¡ˆ

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

**é—®é¢˜ï¼š** Q-learning+ALNSåœ¨ä¸åŒseedä¸‹æ€§èƒ½æ–¹å·®é«˜è¾¾50%ï¼Œéƒ¨åˆ†seedè¡¨ç°ç”šè‡³ä¸å¦‚çº¯matheuristicã€‚

**æ ¹æœ¬åŸå› ï¼š** 5ä¸ªç®—æ³•è®¾è®¡ç¼ºé™·å¯¼è‡´å¯¹åˆå§‹æ¡ä»¶è¿‡äºæ•æ„Ÿã€‚

**è§£å†³æ–¹æ¡ˆï¼š** åˆ†é˜¶æ®µå®æ–½4ä¸ªæ”¹è¿›æ–¹æ¡ˆï¼Œé¢„æœŸé™ä½æ–¹å·®è‡³15%ä»¥å†…ã€‚

---

## ğŸ” é—®é¢˜è¯Šæ–­

### å®éªŒè¯æ®

ä»`docs/data/`ä¸­çš„å®éªŒæ•°æ®åˆ†æï¼š

**å¤±è´¥æ¡ˆä¾‹ (seed 2026):**
```
Largeè§„æ¨¡:
- Baseline matheuristic: 27.14% improvement
- Q-learning: 2.52% improvement
- æ€§èƒ½æš´è·Œ: -24.62% âŒ

Mediumè§„æ¨¡:
- Baseline matheuristic: 54.23% improvement
- Q-learning: 40.08% improvement
- æ€§èƒ½ä¸‹é™: -14.15% âŒ
```

**æˆåŠŸæ¡ˆä¾‹ (seed 2028):**
```
Smallè§„æ¨¡:
- Baseline matheuristic: 32.64% improvement
- Q-learning: 57.74% improvement
- æ€§èƒ½æå‡: +25.10% âœ“
```

### æ ¸å¿ƒé—®é¢˜

#### 1. Epsilonè¡°å‡ç­–ç•¥è¿‡äºæ¿€è¿›

**å½“å‰å®ç°ï¼š**
```python
initial_epsilon: 0.12
epsilon_decay: 0.995
epsilon_min: 0.01

# å®é™…è¡°å‡æ›²çº¿
iteration 100: epsilon â‰ˆ 0.072
iteration 200: epsilon â‰ˆ 0.044
iteration 300: epsilon â‰ˆ 0.027
```

**é—®é¢˜ï¼š**
- 300æ¬¡è¿­ä»£åepsilonæ¥è¿‘æœ€å°å€¼ï¼Œå‡ ä¹å¤±å»æ¢ç´¢èƒ½åŠ›
- å¦‚æœå‰100æ¬¡è¿­ä»£å­¦ä¹ é”™è¯¯ï¼ˆå—seedå½±å“ï¼‰ï¼ŒåæœŸæ— æ³•çº æ­£
- largeè§„æ¨¡ä½¿ç”¨430æ¬¡è¿­ä»£ï¼Œå130æ¬¡å‡ ä¹ä¸æ¢ç´¢

**æ•°å­¦åˆ†æï¼š**
```
æ¢ç´¢æ¬¡æ•°(å‰100æ¬¡) = 100 Ã— 0.09 â‰ˆ 9æ¬¡
æ¢ç´¢æ¬¡æ•°(å200æ¬¡) = 200 Ã— 0.03 â‰ˆ 6æ¬¡
```
å‰æœŸåªæœ‰9æ¬¡çœŸæ­£æ¢ç´¢æœºä¼šï¼Œä¸è¶³ä»¥å­¦ä¹ 4ä¸ªæ“ä½œç¬¦çš„ç»„åˆã€‚

#### 2. åˆå§‹Qå€¼åå·®è¿‡å¤§

**å½“å‰å®ç°ï¼š**
```python
'explore': {'lp': 15.0, 'regret2': 12.0, 'greedy': 10.0}
'stuck': {'lp': 30.0, 'regret2': 12.0, 'greedy': 10.0}
'deep_stuck': {'lp': 35.0, 'regret2': 12.0, 'greedy': 10.0}
```

**é—®é¢˜ï¼š**
- LPçš„åˆå§‹Qå€¼æ˜¯greedyçš„**3.5å€**
- åœ¨epsilon=0.12æ—¶ï¼Œ88%é€‰æ‹©Qå€¼æœ€é«˜çš„LP
- å¦‚æœæŸä¸ªseedçš„åˆå§‹è§£è´¨é‡å·®ï¼ŒLPåœ¨æ—©æœŸè¡¨ç°ä¸å¥½
- Qå€¼è¢«é”™è¯¯æ›´æ–°ä¸ºè´Ÿå€¼ï¼ŒåæœŸepsilonä½æ—¶æ°¸è¿œä¸å†å°è¯•LP

**å®éªŒéªŒè¯ï¼š**
seed 2026 largeè§„æ¨¡å¯èƒ½å‘ç”Ÿï¼š
1. åˆå§‹è§£è´¨é‡å·® â†’ LPå‰æœŸæ•ˆæœä¸å¥½
2. LPçš„Qå€¼ä»35é™åˆ°0ä»¥ä¸‹
3. epsilonè¡°å‡åï¼ŒLPå†ä¹Ÿä¸è¢«é€‰ä¸­
4. æœ€ç»ˆæ€§èƒ½åªæœ‰2.52%

#### 3. å›ºå®šçŠ¶æ€è½¬æ¢é˜ˆå€¼

**å½“å‰å®ç°ï¼š**
```python
stagnation_ratio: 0.16        # stucké˜ˆå€¼ = 16%è¿­ä»£
deep_stagnation_ratio: 0.28   # deep_stuck = 28%è¿­ä»£

# å¯¹300æ¬¡è¿­ä»£
stuck_threshold = 48æ¬¡
deep_stuck_threshold = 84æ¬¡
```

**é—®é¢˜ï¼š**
- ä¸åŒseedçš„æ”¶æ•›é€Ÿåº¦å·®å¼‚å¾ˆå¤§
- å¿«é€Ÿæ”¶æ•›ï¼šè¿‡æ—©è¿›å…¥stuckï¼Œé™åˆ¶æ¢ç´¢
- æ…¢é€Ÿæ”¶æ•›ï¼šè¿‡æ™šè¿›å…¥stuckï¼Œæµªè´¹è¿­ä»£
- æ²¡æœ‰è€ƒè™‘å®é™…å­¦ä¹ è¿›å±•

#### 4. ROIå¥–åŠ±å‡½æ•°è¿‡äºå¤æ‚

**å½“å‰å®ç°ï¼š**
```python
# åŸºç¡€å¥–åŠ±
reward_new_best: 100.0
reward_improvement: 36.0
reward_accepted: 10.0
reward_rejected: -6.0

# ROIç¼©æ”¾
roi_positive_scale: 220.0
roi_negative_scale: 260.0

# æ—¶é—´æƒ©ç½š
time_penalty_threshold: 0.18
time_penalty_positive_scale: 6.5
time_penalty_negative_scale: 14.0
standard_time_penalty_scale: 3.0

# åœºæ™¯ä¹˜æ•°
small: 1.45x
medium: 1.25x
large: 1.0x
```

**é—®é¢˜ï¼š**
- **7ä¸ªç›¸äº’ä½œç”¨çš„è¶…å‚æ•°**
- å‚æ•°ç»„åˆå¯¹æŸäº›seedè¿‡æ‹Ÿåˆ
- æ—¶é—´æƒ©ç½šé€»è¾‘å¤æ‚ï¼Œéš¾ä»¥é¢„æµ‹
- ROIç¼©æ”¾220/260æ— ç†è®ºä¾æ®

**è®¡ç®—å¤æ‚åº¦ç¤ºä¾‹ï¼š**
```python
quality = base_reward + (improvement/cost) Ã— 220 Ã— scenario_multiplier
penalty = action_cost Ã— scale(quality) Ã— scenario_factor
reward = quality - penalty
```
è¿™ç§å¤æ‚åº¦ä½¿å¾—ä¸åŒseedä¸‹çš„å¥–åŠ±ä¿¡å·é«˜åº¦ä¸ç¨³å®šã€‚

#### 5. ç¼ºå°‘æ³›åŒ–æœºåˆ¶

**å½“å‰å®ç°ï¼š**
- å•ä¸€Q-table
- å¯¹åˆå§‹åŒ–æ•æ„Ÿ
- æ²¡æœ‰é›†æˆ/å¹³æ»‘æœºåˆ¶

**é—®é¢˜ï¼š**
- ä¸€æ—¦æŸä¸ªactionçš„Qå€¼è¢«é”™è¯¯æ›´æ–°ï¼Œéš¾ä»¥æ¢å¤
- æ²¡æœ‰æœºåˆ¶å¹³è¡¡ä¸åŒseedçš„è¡¨ç°

---

## ğŸ’¡ æ”¹è¿›æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: è‡ªé€‚åº”Epsilonç­–ç•¥ â­â­â­

**ç›®æ ‡ï¼š** æ ¹æ®å­¦ä¹ è¿›å±•åŠ¨æ€è°ƒæ•´æ¢ç´¢ç‡ï¼Œè€Œéç›²ç›®è¡°å‡ã€‚

**å®ç°ï¼š**

```python
class AdaptiveEpsilonStrategy:
    """è‡ªé€‚åº”epsilonè°ƒæ•´ç­–ç•¥"""

    def __init__(self, initial_epsilon=0.20, epsilon_min=0.05):
        self.initial_epsilon = initial_epsilon  # æé«˜åˆå§‹æ¢ç´¢ç‡
        self.epsilon_min = epsilon_min          # æé«˜æœ€ä½æ¢ç´¢ç‡
        self.recent_improvements = []

    def compute_epsilon(self, iteration, max_iterations,
                       improvement_rate, q_variance):
        """
        åŠ¨æ€è®¡ç®—epsilon

        å‚æ•°:
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            improvement_rate: æœ€è¿‘10æ¬¡è¿­ä»£çš„å¹³å‡æ”¹è¿›ç‡
            q_variance: Qå€¼çš„æ–¹å·®ï¼ˆè¡¡é‡æ”¶æ•›ç¨‹åº¦ï¼‰
        """
        # 1. åŸºç¡€è¡°å‡ï¼ˆæ›´æ¸©å’Œï¼šåªè¡°å‡60%ï¼‰
        progress = iteration / max_iterations
        base_epsilon = self.initial_epsilon * (1 - progress * 0.6)

        # 2. å­¦ä¹ åœæ»æ£€æµ‹
        if improvement_rate < 0.001:  # åœæ» â†’ å¢åŠ æ¢ç´¢
            stagnation_boost = 0.10
        elif improvement_rate > 0.05:  # å¿«é€Ÿå­¦ä¹  â†’ å‡å°‘æ¢ç´¢
            stagnation_boost = -0.05
        else:
            stagnation_boost = 0.0

        # 3. Qå€¼æ”¶æ•›æ£€æµ‹
        if q_variance < 5.0:  # Qå€¼å·²æ”¶æ•› â†’ å¢åŠ æ¢ç´¢
            convergence_boost = 0.08
        else:
            convergence_boost = 0.0

        # 4. å‘¨æœŸæ€§æ¢ç´¢è„‰å†²ï¼ˆæ¯50æ¬¡è¿­ä»£ï¼‰
        pulse_boost = 0.08 if iteration % 50 == 0 else 0.0

        epsilon = base_epsilon + stagnation_boost + convergence_boost + pulse_boost
        return max(self.epsilon_min, min(0.30, epsilon))
```

**ä¼˜åŠ¿ï¼š**
- âœ… å­¦ä¹ åœæ»æ—¶è‡ªåŠ¨å¢åŠ æ¢ç´¢ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜
- âœ… Qå€¼æ”¶æ•›æ—¶å¼ºåˆ¶æ¢ç´¢ï¼Œå‘ç°æ–°ç­–ç•¥
- âœ… å‘¨æœŸæ€§è„‰å†²é˜²æ­¢è¿‡æ—©æ”¶æ•›
- âœ… æœ€ä½5%æ¢ç´¢ç‡ä¿æŒç»ˆèº«å­¦ä¹ èƒ½åŠ›

**é¢„æœŸæ•ˆæœï¼š**
- å‰100æ¬¡è¿­ä»£ï¼šå¹³å‡epsilon=0.15ï¼Œçº¦15æ¬¡æ¢ç´¢
- å200æ¬¡è¿­ä»£ï¼šå¹³å‡epsilon=0.08ï¼Œçº¦16æ¬¡æ¢ç´¢
- æ€»æ¢ç´¢æ¬¡æ•°ä»15æ¬¡æå‡åˆ°31æ¬¡ï¼ˆç¿»å€ï¼‰

### æ–¹æ¡ˆ2: ä¿å®ˆåˆå§‹åŒ–Qå€¼ â­â­â­

**ç›®æ ‡ï¼š** å‡å°å…ˆéªŒåå¥½ï¼Œè®©ç®—æ³•é€šè¿‡å­¦ä¹ å‘ç°æœ€ä¼˜ç­–ç•¥ã€‚

**å®ç°ï¼š**

```python
def _default_q_learning_initial_q_conservative(self):
    """
    ä¿å®ˆçš„åˆå§‹Qå€¼ï¼šç¼©å°å·®è·ï¼Œè®©ç®—æ³•è‡ªå·±å­¦ä¹ 

    åŸå§‹ç‰ˆæœ¬ï¼šLP=35, greedy=10 (3.5å€å·®è·)
    ä¿å®ˆç‰ˆæœ¬ï¼šLP=20, greedy=10 (2.0å€å·®è·)
    """

    base_values = {
        'explore': {
            'lp': 12.0,      # ä»15.0é™ä½
            'regret2': 10.0,
            'greedy': 9.0,
            'random': 5.0,
        },
        'stuck': {
            'lp': 15.0,      # ä»30.0é™ä½
            'regret2': 12.0,
            'greedy': 10.0,
            'random': 5.0,
        },
        'deep_stuck': {
            'lp': 20.0,      # ä»35.0é™ä½
            'regret2': 12.0,
            'greedy': 10.0,
            'random': 5.0,
        },
    }

    initial_values = {}
    for state, repair_map in base_values.items():
        state_values = {}
        for destroy in self._destroy_operators:
            for repair in self.repair_operators:
                value = repair_map.get(repair, 8.0)
                state_values[(destroy, repair)] = value
        initial_values[state] = state_values

    return initial_values
```

**ä¼˜åŠ¿ï¼š**
- âœ… å‡å°‘å¯¹åˆå§‹è§£è´¨é‡çš„ä¾èµ–
- âœ… ç»™Q-learningæ›´å¤šå­¦ä¹ ç©ºé—´
- âœ… ä¸åŒseedæœ‰æ›´ä¸€è‡´çš„èµ·ç‚¹
- âœ… å³ä½¿LPæ—©æœŸè¡¨ç°å·®ï¼Œä¹Ÿä¸ä¼šè¢«å®Œå…¨æ”¾å¼ƒ

**é¢„æœŸæ•ˆæœï¼š**
- LPåœ¨exploreé˜¶æ®µçš„é€‰æ‹©æ¦‚ç‡ä»88%é™åˆ°65%
- å…¶ä»–æ“ä½œç¬¦æœ‰æ›´å¤šæœºä¼šè¢«å°è¯•
- å­¦ä¹ æ›´å¹³è¡¡

### æ–¹æ¡ˆ3: åŠ¨æ€çŠ¶æ€è½¬æ¢ â­â­

**ç›®æ ‡ï¼š** åŸºäºå®é™…å­¦ä¹ è¿›å±•å†³å®šçŠ¶æ€ï¼Œè€Œéå›ºå®šæ¯”ä¾‹ã€‚

**å®ç°ï¼š**

```python
class DynamicStateManager:
    """åŠ¨æ€çŠ¶æ€è½¬æ¢ç®¡ç†å™¨"""

    def __init__(self):
        self.improvement_history = []
        self.q_value_snapshots = []

    def determine_state(self, consecutive_no_improve, iteration,
                       max_iterations, current_q_table):
        """
        åŠ¨æ€å†³å®šå½“å‰çŠ¶æ€

        è€ƒè™‘å› ç´ ï¼š
        1. è¿ç»­æ— æ”¹è¿›æ¬¡æ•°ï¼ˆåŸå§‹æŒ‡æ ‡ï¼‰
        2. å­¦ä¹ é€Ÿåº¦ï¼ˆæ–°æŒ‡æ ‡ï¼‰
        3. Qå€¼æ”¶æ•›åº¦ï¼ˆæ–°æŒ‡æ ‡ï¼‰
        """
        # 1. åŸºç¡€é˜ˆå€¼ï¼ˆæ›´å®½æ¾ï¼‰
        base_stuck = max(20, int(max_iterations * 0.10))   # ä»16%é™åˆ°10%
        base_deep = max(35, int(max_iterations * 0.18))    # ä»28%é™åˆ°18%

        # 2. å­¦ä¹ é€Ÿåº¦è°ƒæ•´
        if len(self.improvement_history) >= 10:
            recent_improvement = sum(self.improvement_history[-10:])

            if recent_improvement < 0.001:  # å‡ ä¹æ— æ”¹è¿› â†’ æå‰è¿›å…¥stuck
                stuck_threshold = int(base_stuck * 0.7)
                deep_threshold = int(base_deep * 0.8)
            elif recent_improvement > 0.1:  # å¿«é€Ÿå­¦ä¹  â†’ å»¶åè¿›å…¥stuck
                stuck_threshold = int(base_stuck * 1.3)
                deep_threshold = int(base_deep * 1.2)
            else:
                stuck_threshold = base_stuck
                deep_threshold = base_deep
        else:
            stuck_threshold = base_stuck
            deep_threshold = base_deep

        # 3. Qå€¼æ”¶æ•›åº¦æ£€æµ‹
        if len(self.q_value_snapshots) >= 5:
            recent_variances = [self._compute_q_variance(q)
                               for q in self.q_value_snapshots[-5:]]
            avg_variance = np.mean(recent_variances)

            if avg_variance < 3.0:  # Qå€¼å·²æ”¶æ•› â†’ å¼ºåˆ¶deep exploration
                return 'deep_stuck'

        # 4. å¸¸è§„åˆ¤æ–­
        if consecutive_no_improve >= deep_threshold:
            return 'deep_stuck'
        elif consecutive_no_improve >= stuck_threshold:
            return 'stuck'
        else:
            return 'explore'

    def _compute_q_variance(self, q_table):
        """è®¡ç®—Qè¡¨çš„æ–¹å·®"""
        all_values = []
        for state_values in q_table.values():
            all_values.extend(state_values.values())
        return np.var(all_values) if all_values else 0.0

    def record_improvement(self, improvement):
        """è®°å½•æ”¹è¿›é‡"""
        self.improvement_history.append(improvement)
        if len(self.improvement_history) > 20:
            self.improvement_history.pop(0)

    def record_q_snapshot(self, q_table):
        """è®°å½•Qè¡¨å¿«ç…§"""
        snapshot = {state: dict(values)
                   for state, values in q_table.items()}
        self.q_value_snapshots.append(snapshot)
        if len(self.q_value_snapshots) > 10:
            self.q_value_snapshots.pop(0)
```

**ä¼˜åŠ¿ï¼š**
- âœ… é€‚åº”ä¸åŒæ”¶æ•›é€Ÿåº¦
- âœ… é¿å…è¿‡æ—©æˆ–è¿‡æ™šçš„çŠ¶æ€è½¬æ¢
- âœ… Qå€¼æ”¶æ•›æ—¶å¼ºåˆ¶deep exploration
- âœ… æ›´æ™ºèƒ½çš„çŠ¶æ€ç®¡ç†

### æ–¹æ¡ˆ4: ç®€åŒ–å¥–åŠ±å‡½æ•° â­â­â­

**ç›®æ ‡ï¼š** å‡å°‘è¶…å‚æ•°ï¼Œæé«˜é²æ£’æ€§ã€‚

**å®ç°ï¼š**

```python
def _compute_q_reward_simplified(
    self,
    improvement: float,
    is_new_best: bool,
    is_accepted: bool,
    action_cost: float,
    repair_operator: str,
    previous_cost: float,
):
    """
    ç®€åŒ–çš„å¥–åŠ±å‡½æ•°ï¼šåªä¿ç•™æ ¸å¿ƒä¿¡å·

    ç§»é™¤ï¼š
    - ROIè¶…å‚æ•°ï¼ˆ220/260ï¼‰
    - å¤æ‚çš„æ—¶é—´æƒ©ç½šç¼©æ”¾
    - åœºæ™¯ç‰¹å®šä¹˜æ•°

    ä¿ç•™ï¼š
    - è´¨é‡åˆ†çº§å¥–åŠ±
    - ç›¸å¯¹æ”¹è¿›å¥–åŠ±
    - æ¸©å’Œçš„æ—¶é—´æƒ©ç½š
    """

    # 1. è´¨é‡å¥–åŠ±ï¼ˆ3æ¡£ï¼‰
    if is_new_best:
        quality_reward = 100.0
    elif improvement > 0:
        # ç›¸å¯¹æ”¹è¿›å¥–åŠ±ï¼ˆè‡ªç„¶ç¼©æ”¾ï¼Œæ— éœ€è¶…å‚æ•°ï¼‰
        relative_improvement = improvement / max(previous_cost, 1.0)
        # çº¿æ€§æ˜ å°„ï¼š1%æ”¹è¿›â†’5åˆ†ï¼Œ10%æ”¹è¿›â†’50åˆ†
        quality_reward = min(50.0, relative_improvement * 500.0)
    elif is_accepted:
        quality_reward = 5.0
    else:
        quality_reward = -5.0

    # 2. æ—¶é—´æƒ©ç½šï¼ˆåªé’ˆå¯¹matheuristicï¼Œä¸”æ¸©å’Œï¼‰
    time_penalty = 0.0
    is_matheuristic = repair_operator in {'lp'}

    if is_matheuristic and action_cost > 0.5:
        # æ¸©å’Œæƒ©ç½šï¼šæ…¢æ“ä½œæœ€å¤šæ‰£20åˆ†
        # ä½†å¦‚æœæ‰¾åˆ°æ–°æœ€ä¼˜ï¼Œä¸æƒ©ç½š
        if is_new_best:
            time_penalty = 0.0
        else:
            time_penalty = min(20.0, action_cost * 10.0)

    return quality_reward - time_penalty
```

**å¯¹æ¯”ï¼š**

| æŒ‡æ ‡ | åŸå§‹ç‰ˆæœ¬ | ç®€åŒ–ç‰ˆæœ¬ |
|------|---------|---------|
| è¶…å‚æ•°æ•°é‡ | 7ä¸ª | 0ä¸ª |
| æœ€å¤§å¥–åŠ± | ~150 | 100 |
| æœ€å¤§æƒ©ç½š | ~-100 | -20 |
| è®¡ç®—æ­¥éª¤ | 5æ­¥ | 2æ­¥ |
| å¯é¢„æµ‹æ€§ | ä½ | é«˜ |

**ä¼˜åŠ¿ï¼š**
- âœ… ç§»é™¤æ‰€æœ‰æ‰‹å·¥è°ƒæ•´çš„è¶…å‚æ•°
- âœ… ç›¸å¯¹æ”¹è¿›è‡ªç„¶ç¼©æ”¾ï¼Œæ— éœ€ROIå‚æ•°
- âœ… æ—¶é—´æƒ©ç½šæ›´æ¸©å’Œåˆç†
- âœ… æé«˜è·¨seedæ³›åŒ–èƒ½åŠ›

---

## ğŸ“Š å®æ–½è®¡åˆ’

### Phase 1: å¿«é€ŸéªŒè¯ï¼ˆ1-2å¤©ï¼‰

**ç›®æ ‡ï¼š** éªŒè¯æ–¹æ¡ˆ1+2+4çš„ç»„åˆæ•ˆæœ

**æ­¥éª¤ï¼š**
1. ä¿®æ”¹`src/config/defaults.py`çš„Q-learningå‚æ•°
2. å®ç°è‡ªé€‚åº”epsilonï¼ˆæ–¹æ¡ˆ1ï¼‰
3. æ›´æ–°åˆå§‹Qå€¼ï¼ˆæ–¹æ¡ˆ2ï¼‰
4. ç®€åŒ–å¥–åŠ±å‡½æ•°ï¼ˆæ–¹æ¡ˆ4ï¼‰
5. æµ‹è¯•seeds 2026, 2028, 2031, 2034ï¼ˆ4ä¸ªseedï¼‰

**éªŒæ”¶æ ‡å‡†ï¼š**
- seed 2026 largeè§„æ¨¡ï¼šä»2.52%æå‡åˆ°è‡³å°‘15%
- 4ä¸ªseedçš„æ€§èƒ½æ–¹å·®ï¼šä»50%é™ä½åˆ°30%ä»¥å†…

**ä»£ç æ”¹åŠ¨ï¼š**
```python
# src/config/defaults.py
@dataclass
class QLearningParams:
    alpha: float = 0.35
    gamma: float = 0.95
    initial_epsilon: float = 0.20        # ä»0.12æé«˜
    epsilon_decay: float = 0.998         # ä»0.995å‡ç¼“
    epsilon_min: float = 0.05            # ä»0.01æé«˜
    enable_online_updates: bool = True

    # ç®€åŒ–çš„å¥–åŠ±å‚æ•°
    reward_new_best: float = 100.0
    reward_improvement: float = 50.0     # ç®€åŒ–ï¼šä¸å†éœ€è¦ROIç¼©æ”¾
    reward_accepted: float = 5.0         # ä»10.0é™ä½
    reward_rejected: float = -5.0        # ä»-6.0è°ƒæ•´

    # ç§»é™¤ROIè¶…å‚æ•°
    # roi_positive_scale: float = 220.0  # REMOVED
    # roi_negative_scale: float = 260.0  # REMOVED

    # ç®€åŒ–æ—¶é—´æƒ©ç½š
    time_penalty_threshold: float = 0.5  # åªæƒ©ç½šçœŸæ­£æ…¢çš„
    time_penalty_scale: float = 10.0     # ç»Ÿä¸€ç¼©æ”¾

    # æ›´å®½æ¾çš„çŠ¶æ€è½¬æ¢
    stagnation_ratio: float = 0.10       # ä»0.16é™ä½
    deep_stagnation_ratio: float = 0.18  # ä»0.28é™ä½
    stagnation_threshold: int = 20       # ä»30é™ä½
    deep_stagnation_threshold: int = 35  # ä»45é™ä½
```

### Phase 2: å…¨é¢æµ‹è¯•ï¼ˆ3-5å¤©ï¼‰

**ç›®æ ‡ï¼š** åœ¨10ä¸ªseedä¸ŠéªŒè¯ç¨³å®šæ€§

**æ­¥éª¤ï¼š**
1. è¿è¡Œå®Œæ•´çš„10-seedæµ‹è¯•
2. æ”¶é›†æ€§èƒ½æ•°æ®å’Œæ–¹å·®
3. å¯¹æ¯”baseline matheuristic
4. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š

**éªŒæ”¶æ ‡å‡†ï¼š**
- 10ä¸ªseedçš„å¹³å‡æ€§èƒ½ â‰¥ matheuristic
- æ€§èƒ½æ–¹å·® â‰¤ 15%
- æœ€å·®seedçš„æ€§èƒ½ â‰¥ matheuristicçš„80%

### Phase 3: é«˜çº§ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

**ç›®æ ‡ï¼š** å®æ–½æ–¹æ¡ˆ3ï¼ˆåŠ¨æ€çŠ¶æ€ï¼‰æˆ–æ–¹æ¡ˆ5ï¼ˆé›†æˆå­¦ä¹ ï¼‰

**æ­¥éª¤ï¼š**
1. å®ç°åŠ¨æ€çŠ¶æ€ç®¡ç†å™¨
2. æ·»åŠ è¿è¡Œæ—¶æŒ‡æ ‡æ”¶é›†
3. A/Bæµ‹è¯•å¯¹æ¯”Phase 1ç»“æœ
4. é€‰æ‹©æ€§èƒ½æœ€å¥½çš„ç‰ˆæœ¬

---

## ğŸ§ª å®éªŒéªŒè¯åè®®

### æµ‹è¯•é…ç½®

```python
TEST_SEEDS = [2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034]
TEST_SCALES = ['small', 'medium', 'large']
TEST_METHODS = ['minimal', 'matheuristic', 'q_learning']
```

### è¯„ä¼°æŒ‡æ ‡

1. **å¹³å‡æ”¹è¿›ç‡**ï¼š
   ```
   avg_improvement = mean(improvement_rates across all seeds)
   ```

2. **æ€§èƒ½æ–¹å·®**ï¼š
   ```
   variance = std(improvement_rates) / mean(improvement_rates)
   ```

3. **ç¨³å®šæ€§å¾—åˆ†**ï¼š
   ```
   stability = 1 - (worst_case / best_case)
   ```

4. **vs Matheuristicç›¸å¯¹æ€§èƒ½**ï¼š
   ```
   relative = q_learning_improvement / matheuristic_improvement
   ```

### æˆåŠŸæ ‡å‡†

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ |
|------|------|------|
| å¹³å‡æ”¹è¿›ç‡ (large) | ~15% | â‰¥25% |
| æ€§èƒ½æ–¹å·® | ~50% | â‰¤15% |
| ç¨³å®šæ€§å¾—åˆ† | ~0.5 | â‰¥0.85 |
| vs Matheuristic | ~0.8 | â‰¥1.1 |

---

## ğŸ“ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³è¡ŒåŠ¨ï¼ˆä»Šå¤©ï¼‰

1. **å¤‡ä»½å½“å‰ä»£ç **
   ```bash
   git commit -am "Backup before Q-learning stability fixes"
   ```

2. **å®æ–½Phase 1æ”¹åŠ¨**
   - ä¿®æ”¹`src/config/defaults.py`
   - æ›´æ–°`src/planner/alns.py`çš„åˆå§‹Qå€¼å‡½æ•°
   - ç®€åŒ–å¥–åŠ±è®¡ç®—å‡½æ•°

3. **å¿«é€Ÿæµ‹è¯•2ä¸ªseed**
   ```bash
   python scripts/generate_alns_visualization.py --seed 2026
   python scripts/generate_alns_visualization.py --seed 2028
   ```

### æ˜å¤©

4. **å®æ–½è‡ªé€‚åº”epsilon**
   - åœ¨`src/planner/q_learning.py`æ·»åŠ `AdaptiveEpsilonStrategy`ç±»
   - ä¿®æ”¹`MinimalALNS`ä½¿ç”¨æ–°ç­–ç•¥

5. **å®Œæ•´æµ‹è¯•4ä¸ªseed**

### æœ¬å‘¨

6. **Phase 2å®Œæ•´éªŒè¯**
7. **å‡†å¤‡è®ºæ–‡ææ–™**

---

## ğŸ“ ç†è®ºä¾æ®

è¿™äº›æ”¹è¿›æ–¹æ¡ˆåŸºäºä»¥ä¸‹å¼ºåŒ–å­¦ä¹ ç†è®ºï¼š

### 1. Exploration-Exploitation Trade-off

**æ–‡çŒ®ï¼š** Sutton & Barto (2018), "Reinforcement Learning: An Introduction"

**ç†è®ºï¼š** epsilon-greedyç­–ç•¥éœ€è¦åœ¨æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹ä¸­ä¿æŒä¸€å®šæ¢ç´¢ç‡ã€‚

**æˆ‘ä»¬çš„æ”¹è¿›ï¼š**
- è‡ªé€‚åº”epsilonä¿è¯æŒç»­æ¢ç´¢
- å‘¨æœŸæ€§è„‰å†²é˜²æ­¢premature convergence

### 2. Optimistic Initialization

**æ–‡çŒ®ï¼š** Thrun (1992), "Efficient exploration in reinforcement learning"

**ç†è®ºï¼š** åˆå§‹Qå€¼åº”è¯¥ä¹è§‚ä½†ä¸è¿‡åˆ†ï¼Œé¼“åŠ±æ—©æœŸæ¢ç´¢ã€‚

**æˆ‘ä»¬çš„æ”¹è¿›ï¼š**
- ä¿å®ˆåˆå§‹åŒ–å‡å°‘åå·®
- è®©å­¦ä¹ å‘ç°çœŸå®ä»·å€¼

### 3. Reward Shaping

**æ–‡çŒ®ï¼š** Ng et al. (1999), "Policy invariance under reward transformations"

**ç†è®ºï¼š** å¥–åŠ±å‡½æ•°åº”è¯¥ç®€å•ç¨³å®šï¼Œé¿å…å¤æ‚éçº¿æ€§å˜æ¢ã€‚

**æˆ‘ä»¬çš„æ”¹è¿›ï¼š**
- ç§»é™¤ROIè¶…å‚æ•°
- ä½¿ç”¨ç›¸å¯¹æ”¹è¿›çš„è‡ªç„¶ç¼©æ”¾

### 4. State Aggregation

**æ–‡çŒ®ï¼š** Singh et al. (1995), "Reinforcement learning with soft state aggregation"

**ç†è®ºï¼š** çŠ¶æ€å®šä¹‰åº”è¯¥åŸºäºå®é™…åŠ¨æ€ï¼Œè€Œéå›ºå®šè§„åˆ™ã€‚

**æˆ‘ä»¬çš„æ”¹è¿›ï¼š**
- åŠ¨æ€çŠ¶æ€è½¬æ¢
- åŸºäºå­¦ä¹ è¿›å±•è°ƒæ•´

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.

2. Thrun, S. (1992). *Efficient exploration in reinforcement learning*. Carnegie Mellon University.

3. Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under reward transformations: Theory and application to reward shaping. *ICML*, 99, 278-287.

4. Singh, S. P., Jaakkola, T., & Jordan, M. I. (1995). Reinforcement learning with soft state aggregation. *Advances in neural information processing systems*, 7.

5. Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. *Nature*, 518(7540), 529-533.

---

## é™„å½•A: å®Œæ•´ä»£ç ä¿®æ”¹æ¸…å•

### 1. src/config/defaults.py

```python
@dataclass
class QLearningParams:
    """Phase 1 Stability Fix: Conservative and Adaptive Parameters"""

    # Learning parameters
    alpha: float = 0.35
    gamma: float = 0.95

    # Adaptive epsilon (Phase 1 improvement)
    initial_epsilon: float = 0.20        # â†‘ from 0.12
    epsilon_decay: float = 0.998         # â†“ from 0.995 (slower decay)
    epsilon_min: float = 0.05            # â†‘ from 0.01
    enable_online_updates: bool = True

    # Simplified rewards (Phase 1 improvement)
    reward_new_best: float = 100.0
    reward_improvement: float = 50.0     # Simplified, no ROI needed
    reward_accepted: float = 5.0
    reward_rejected: float = -5.0

    # Gentle time penalty (Phase 1 improvement)
    time_penalty_threshold: float = 0.5
    time_penalty_scale: float = 10.0

    # Relaxed state transitions (Phase 1 improvement)
    stagnation_ratio: float = 0.10       # â†“ from 0.16
    deep_stagnation_ratio: float = 0.18  # â†“ from 0.28
    stagnation_threshold: int = 20
    deep_stagnation_threshold: int = 35
```

### 2. src/planner/alns.py

åœ¨`_default_q_learning_initial_q`æ–¹æ³•ä¸­æ›¿æ¢ä¸ºä¿å®ˆåˆå§‹åŒ–ï¼š

```python
def _default_q_learning_initial_q(self) -> Dict[str, Dict[Action, float]]:
    """Conservative initialization: reduce LP bias"""

    base_values = {
        'explore': {
            'lp': 12.0,      # â†“ from 15.0
            'regret2': 10.0,
            'greedy': 9.0,
            'random': 5.0,
        },
        'stuck': {
            'lp': 15.0,      # â†“ from 30.0
            'regret2': 12.0,
            'greedy': 10.0,
            'random': 5.0,
        },
        'deep_stuck': {
            'lp': 20.0,      # â†“ from 35.0
            'regret2': 12.0,
            'greedy': 10.0,
            'random': 5.0,
        },
    }

    # ... rest of the method
```

åœ¨`_compute_q_reward`æ–¹æ³•ä¸­ç®€åŒ–è®¡ç®—ï¼š

```python
def _compute_q_reward(
    self,
    improvement: float,
    is_new_best: bool,
    is_accepted: bool,
    action_cost: float,
    repair_operator: str,
    previous_cost: float,
) -> float:
    """Simplified reward function (Phase 1)"""

    params = self._q_params or DEFAULT_Q_LEARNING_PARAMS

    # 1. Quality reward (3-tier)
    if is_new_best:
        quality = params.reward_new_best
    elif improvement > 0:
        relative = improvement / max(previous_cost, 1.0)
        quality = min(params.reward_improvement, relative * 500.0)
    elif is_accepted:
        quality = params.reward_accepted
    else:
        quality = params.reward_rejected

    # 2. Gentle time penalty (only for matheuristic)
    penalty = 0.0
    is_matheuristic = repair_operator in self._matheuristic_repairs

    if is_matheuristic and action_cost > params.time_penalty_threshold:
        if is_new_best:
            penalty = 0.0  # No penalty for finding new best
        else:
            penalty = min(20.0, action_cost * params.time_penalty_scale)

    return quality - penalty
```

---

## ç»“è®º

é€šè¿‡ç³»ç»Ÿæ€§çš„ç®—æ³•æ”¹è¿›ï¼Œæˆ‘ä»¬æœ‰æœ›å°†Q-learningçš„æ€§èƒ½æ–¹å·®ä»50%é™ä½åˆ°15%ä»¥å†…ï¼ŒåŒæ—¶ä¿æŒæˆ–è¶…è¶Šmatheuristicçš„å¹³å‡æ€§èƒ½ã€‚è¿™äº›æ”¹è¿›ä¸ä»…æé«˜äº†ç®—æ³•çš„é²æ£’æ€§ï¼Œä¹Ÿä¸ºè®ºæ–‡æä¾›äº†æœ‰ä»·å€¼çš„methodological contributionsã€‚
