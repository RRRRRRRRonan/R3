# Scale-Aware Q-Learning (SAQL) è¯¦ç»†å‘¨è®¡åˆ’

**åˆ›å»ºæ—¥æœŸ**: 2025-11-09
**é¡¹ç›®**: ç”µåŠ¨è½¦è·¯å¾„è§„åˆ’é—®é¢˜ï¼ˆE-VRPï¼‰è§„æ¨¡è‡ªé€‚åº”Q-learningä¼˜åŒ–
**ç ”ç©¶æ–¹å‘**: è§„æ¨¡è‡ªé€‚åº”Q-learning (SAQL) + åŠ¨æ€åœ¨çº¿ä¼˜åŒ–
**ç›®æ ‡**: Q2+æœŸåˆŠå‘è¡¨

---

## ç›®å½•

- [å››ä¸ªæ ¸å¿ƒQ-learningé—®é¢˜æ€»è§ˆ](#å››ä¸ªæ ¸å¿ƒq-learningé—®é¢˜æ€»è§ˆ)
- [Phase 1: ä¿®å¤Q-learningæ ¸å¿ƒé—®é¢˜ (Week 1-7)](#phase-1-ä¿®å¤q-learningæ ¸å¿ƒé—®é¢˜-week-1-7)
- [Phase 2: åŠ¨æ€E-VRPåœ¨çº¿ä¼˜åŒ– (Week 8-13)](#phase-2-åŠ¨æ€e-vrpåœ¨çº¿ä¼˜åŒ–-week-8-13)
- [Phase 3: å…¨é¢å®éªŒä¸è®ºæ–‡å†™ä½œ (Week 14-17)](#phase-3-å…¨é¢å®éªŒä¸è®ºæ–‡å†™ä½œ-week-14-17)
- [Phase 4: ä¿®è®¢ä¸æŠ•ç¨¿ (Week 18-21)](#phase-4-ä¿®è®¢ä¸æŠ•ç¨¿-week-18-21)
- [æ¯å‘¨æ£€æŸ¥æ¸…å•](#æ¯å‘¨æ£€æŸ¥æ¸…å•)

---

## å››ä¸ªæ ¸å¿ƒQ-learningé—®é¢˜æ€»è§ˆ

### å½“å‰å­˜åœ¨çš„é—®é¢˜

| é—®é¢˜ç¼–å· | é—®é¢˜æè¿° | å½“å‰çŠ¶æ€ | ä»£ç ä½ç½® | è§£å†³å‘¨æ¬¡ |
|---------|---------|---------|---------|---------|
| **é—®é¢˜1** | Q-tableåˆå§‹åŒ–ä¸º0.0 | ä¸é¼“åŠ±æ—©æœŸæ¢ç´¢ | `src/planner/q_learning.py:64-66` | Week 1-2 |
| **é—®é¢˜2** | çŠ¶æ€ç©ºé—´åªæœ‰3ä¸ªçŠ¶æ€ | å¤ªç²—ç³™ï¼Œæ— æ³•æ•æ‰ç»†èŠ‚ | `src/config/defaults.py` | Week 3-4 |
| **é—®é¢˜3** | Epsilonå›ºå®šä¸º0.12 | å¤§è§„æ¨¡é—®é¢˜æ¢ç´¢ä¸è¶³ | `src/config/defaults.py` | Week 2 + Week 6 |
| **é—®é¢˜4** | å¥–åŠ±æœªå½’ä¸€åŒ– | è·¨è§„æ¨¡å­¦ä¹ ä¸ç¨³å®š | `src/planner/alns.py:623-696` | Week 5 |

### é—®é¢˜å½±å“åˆ†æ

**å½“å‰æ€§èƒ½é—®é¢˜**:
- å°è§„æ¨¡ï¼š62.45% æ”¹è¿›ç‡ âœ“
- å¤§è§„æ¨¡ï¼š6.92% æ”¹è¿›ç‡ âŒ (Matheuristic: 27.05%)
- ç§å­æ–¹å·®ï¼šæé«˜ï¼ˆ6.92% ~ 38.31%ï¼‰

**æ ¹æœ¬åŸå› **:
1. Qå€¼å…¨ä¸º0 â†’ æ— æ¢ç´¢åå¥½ â†’ æ—©æœŸé™·å…¥å±€éƒ¨æœ€ä¼˜
2. 3çŠ¶æ€ç©ºé—´ â†’ å­¦ä¸åˆ°ç»†ç²’åº¦ç­–ç•¥
3. ä½epsilon â†’ å¤§è§„æ¨¡é—®é¢˜æ¢ç´¢ä¸å¤Ÿ
4. å¥–åŠ±ä¸å½’ä¸€åŒ– â†’ Qå€¼å­¦ä¹ æ··ä¹±

**ç›®æ ‡æ”¹è¿›**:
- å¤§è§„æ¨¡ï¼šä» 7% æå‡åˆ° 25%+
- ç§å­æ–¹å·®ï¼šé™ä½ 60%+
- åŠ¨æ€å“åº”ï¼š< 1ç§’

---

# Phase 1: ä¿®å¤Q-learningæ ¸å¿ƒé—®é¢˜ (Week 1-7)

---

## Week 1: åŸºçº¿æ”¶é›† + Q-tableåˆå§‹åŒ–å®éªŒï¼ˆé—®é¢˜1ï¼‰

### ğŸ¯ æœ¬å‘¨ç›®æ ‡
1. å»ºç«‹å½“å‰Q-learningçš„æ€§èƒ½åŸºçº¿ï¼ˆ10ç§å­ï¼‰
2. æµ‹è¯•4ç§Q-tableåˆå§‹åŒ–ç­–ç•¥
3. ç¡®å®šæœ€ä¼˜åˆå§‹åŒ–æ–¹æ¡ˆ

### ğŸ“‹ é—®é¢˜1ï¼šQ-tableåˆå§‹åŒ–ä¸º0.0

**å½“å‰ä»£ç é—®é¢˜** (`src/planner/q_learning.py:64-66`):
```python
self.q_table: Dict[State, Dict[Action, float]] = {
    state: {action: 0.0 for action in self.actions} for state in self.states
}
```

**é—®é¢˜åˆ†æ**:
- æ‰€æœ‰Qå€¼åˆå§‹åŒ–ä¸º0ï¼Œæ²¡æœ‰æ¢ç´¢åå¥½
- ä¸æ˜¯å£°ç§°çš„"zero-bias initialization"
- å¯¼è‡´ç®—æ³•è¿‡æ—©æ”¶æ•›

---

### ğŸ“… Day 1-3: å¤šç§å­åŸºçº¿æ”¶é›†

**ä»»åŠ¡**: è¿è¡Œå½“å‰å®ç°ï¼Œæ”¶é›†10ä¸ªç§å­çš„æ€§èƒ½æ•°æ®

**æ‰§è¡Œæ­¥éª¤**:

1. **åˆ›å»ºå®éªŒè„šæœ¬** `scripts/week1_baseline_collection.sh`:
```bash
#!/bin/bash
# Week 1 åŸºçº¿æ•°æ®æ”¶é›†

SEEDS=(2025 2026 2027 2028 2029 2030 2031 2032 2033 2034)
SCENARIOS=("small" "medium" "large")

for scenario in "${SCENARIOS[@]}"; do
    for seed in "${SEEDS[@]}"; do
        echo "Running ${scenario} with seed ${seed}..."
        python scripts/run_alns_preset.py \
            --scenario ${scenario} \
            --solver q_learning \
            --seed ${seed} \
            --output results/week1/baseline_${scenario}_seed${seed}.json
    done
done

echo "Baseline collection complete!"
```

2. **è¿è¡Œå®éªŒ**:
```bash
chmod +x scripts/week1_baseline_collection.sh
./scripts/week1_baseline_collection.sh
```

3. **æ•°æ®æ”¶é›†**:
   - é¢„æœŸè¾“å‡ºï¼š30ä¸ªç»“æœæ–‡ä»¶ï¼ˆ3è§„æ¨¡ Ã— 10ç§å­ï¼‰
   - å­˜å‚¨ä½ç½®ï¼š`results/week1/baseline_*.json`

4. **ç»Ÿè®¡åˆ†æè„šæœ¬** `scripts/analyze_baseline.py`:
```python
import json
import numpy as np
from pathlib import Path
from scipy import stats

def analyze_baseline(results_dir: str = "results/week1"):
    """åˆ†æåŸºçº¿ç»“æœ"""

    results = {"small": [], "medium": [], "large": []}

    # è¯»å–æ‰€æœ‰ç»“æœæ–‡ä»¶
    for file in Path(results_dir).glob("baseline_*.json"):
        with open(file) as f:
            data = json.load(f)
            scale = data["scenario"]
            improvement = data["improvement_ratio"]
            results[scale].append(improvement)

    # ç»Ÿè®¡åˆ†æ
    print("=" * 60)
    print("åŸºçº¿æ€§èƒ½åˆ†æ")
    print("=" * 60)

    for scale, improvements in results.items():
        arr = np.array(improvements)
        print(f"\n{scale.upper()} è§„æ¨¡:")
        print(f"  å¹³å‡æ”¹è¿›ç‡: {arr.mean():.2%} Â± {arr.std():.2%}")
        print(f"  æœ€å°/æœ€å¤§: {arr.min():.2%} / {arr.max():.2%}")
        print(f"  å˜å¼‚ç³»æ•° (CV): {arr.std() / arr.mean():.3f}")
        print(f"  æ ·æœ¬æ•°: {len(arr)}")

    # ä¿å­˜æ±‡æ€»
    summary = {
        scale: {
            "mean": float(np.mean(improvements)),
            "std": float(np.std(improvements)),
            "min": float(np.min(improvements)),
            "max": float(np.max(improvements)),
            "cv": float(np.std(improvements) / np.mean(improvements))
        }
        for scale, improvements in results.items()
    }

    with open(f"{results_dir}/baseline_summary.json", "w") as f:
        json.dump(summary, f, indent=2)

    return summary

if __name__ == "__main__":
    analyze_baseline()
```

**é¢„æœŸç»“æœ**:
- å°è§„æ¨¡ï¼šå‡å€¼ ~60%ï¼ŒCV ~0.15
- ä¸­è§„æ¨¡ï¼šå‡å€¼ ~30%ï¼ŒCV ~0.25
- å¤§è§„æ¨¡ï¼šå‡å€¼ ~7%ï¼ŒCV ~0.40ï¼ˆé«˜æ–¹å·®ï¼ï¼‰

---

### ğŸ“… Day 4-7: Q-tableåˆå§‹åŒ–ç­–ç•¥å®éªŒ

**ä»»åŠ¡**: æµ‹è¯•4ç§åˆå§‹åŒ–ç­–ç•¥ï¼Œæ‰¾å‡ºæœ€ä¼˜æ–¹æ¡ˆ

#### Step 1: ä¿®æ”¹Q-learningä»£ç æ”¯æŒä¸åŒåˆå§‹åŒ–

**åˆ›å»ºæ–°æ–‡ä»¶** `src/planner/q_learning_init.py`:

```python
"""Q-table initialization strategies"""
from typing import Dict, Callable
from enum import Enum

class QInitStrategy(Enum):
    """Q-tableåˆå§‹åŒ–ç­–ç•¥"""
    ZERO = "zero"              # å½“å‰ï¼šå…¨0åˆå§‹åŒ–
    UNIFORM = "uniform"        # å‡åŒ€æ­£åç½®
    ACTION_SPECIFIC = "action_specific"  # åŠ¨ä½œç‰¹å®š
    STATE_SPECIFIC = "state_specific"    # çŠ¶æ€ç‰¹å®š

def init_zero(state: str, action: tuple, states: tuple, actions: list) -> float:
    """ç­–ç•¥Aï¼šé›¶åˆå§‹åŒ–ï¼ˆå½“å‰æ–¹æ³•ï¼‰"""
    return 0.0

def init_uniform(state: str, action: tuple, states: tuple, actions: list,
                 bias: float = 50.0) -> float:
    """ç­–ç•¥Bï¼šå‡åŒ€æ­£åç½®

    Args:
        bias: æ‰€æœ‰Qå€¼çš„åˆå§‹åç½®ï¼ˆé»˜è®¤50.0ï¼‰

    åŸç†ï¼šæ­£åç½®é¼“åŠ±æ¢ç´¢æ‰€æœ‰åŠ¨ä½œ
    """
    return bias

def init_action_specific(state: str, action: tuple, states: tuple,
                         actions: list) -> float:
    """ç­–ç•¥Cï¼šåŠ¨ä½œç‰¹å®šåˆå§‹åŒ–

    åŸç†ï¼šç»™å·²çŸ¥å¥½çš„ç®—å­ï¼ˆmatheuristicä¿®å¤ï¼‰æ›´é«˜çš„åˆå§‹Qå€¼
    """
    destroy_op, repair_op = action

    # Matheuristicä¿®å¤ç®—å­ç»™æ›´é«˜åˆå§‹å€¼
    if repair_op in ["greedy_lp", "segments"]:
        return 100.0
    else:
        return 50.0

def init_state_specific(state: str, action: tuple, states: tuple,
                        actions: list) -> float:
    """ç­–ç•¥Dï¼šçŠ¶æ€ç‰¹å®šåˆå§‹åŒ–

    åŸç†ï¼šä¸åŒçŠ¶æ€éœ€è¦ä¸åŒçš„æ¿€è¿›ç¨‹åº¦
    """
    state_bias = {
        "explore": 30.0,       # æ—©æœŸæ¢ç´¢ï¼Œä½ä¼˜å…ˆçº§
        "stuck": 70.0,         # å›°ä½æ—¶éœ€è¦æ›´æ¿€è¿›
        "deep_stuck": 120.0    # æ·±åº¦å›°ä½æ—¶æœ€æ¿€è¿›
    }

    return state_bias.get(state, 50.0)

# ç­–ç•¥æ˜ å°„
INIT_STRATEGIES: Dict[QInitStrategy, Callable] = {
    QInitStrategy.ZERO: init_zero,
    QInitStrategy.UNIFORM: init_uniform,
    QInitStrategy.ACTION_SPECIFIC: init_action_specific,
    QInitStrategy.STATE_SPECIFIC: init_state_specific,
}
```

#### Step 2: æ›´æ–°Q-learning Agent

**ä¿®æ”¹æ–‡ä»¶** `src/planner/q_learning.py`:

```python
# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å¯¼å…¥
from planner.q_learning_init import QInitStrategy, INIT_STRATEGIES

class QLearningOperatorAgent:
    """Q-learning agent with configurable initialization"""

    def __init__(
        self,
        destroy_operators: Iterable[str],
        repair_operators: Sequence[str],
        params: QLearningParams,
        *,
        state_labels: Optional[Tuple[str, ...]] = None,
        init_strategy: QInitStrategy = QInitStrategy.ZERO,  # NEW
    ):
        # ... ç°æœ‰ä»£ç  ...

        self.init_strategy = init_strategy

        # åˆå§‹åŒ–Qè¡¨ï¼ˆä½¿ç”¨é€‰å®šçš„ç­–ç•¥ï¼‰
        self.q_table = self._initialize_q_table()

    def _initialize_q_table(self) -> Dict[State, Dict[Action, float]]:
        """ä½¿ç”¨æŒ‡å®šç­–ç•¥åˆå§‹åŒ–Qè¡¨"""

        init_func = INIT_STRATEGIES[self.init_strategy]

        q_table = {}
        for state in self.states:
            q_table[state] = {}
            for action in self.actions:
                q_value = init_func(
                    state=state,
                    action=action,
                    states=self.states,
                    actions=self.actions
                )
                q_table[state][action] = q_value

        return q_table
```

#### Step 3: åˆ›å»ºå®éªŒè„šæœ¬

**åˆ›å»º** `scripts/week1_init_experiments.sh`:

```bash
#!/bin/bash
# Week 1: Q-tableåˆå§‹åŒ–å®éªŒ

SEEDS=(2025 2026 2027 2028 2029 2030 2031 2032 2033 2034)
SCENARIOS=("small" "medium" "large")
STRATEGIES=("zero" "uniform" "action_specific" "state_specific")

for strategy in "${STRATEGIES[@]}"; do
    for scenario in "${SCENARIOS[@]}"; do
        for seed in "${SEEDS[@]}"; do
            echo "Running ${strategy} on ${scenario} with seed ${seed}..."
            python scripts/run_alns_preset.py \
                --scenario ${scenario} \
                --solver q_learning \
                --init_strategy ${strategy} \
                --seed ${seed} \
                --output results/week1/init_${strategy}_${scenario}_seed${seed}.json
        done
    done
done

echo "Initialization experiments complete!"
```

**è¿è¡Œå®éªŒ**:
```bash
chmod +x scripts/week1_init_experiments.sh
./scripts/week1_init_experiments.sh
```

**é¢„æœŸè¿è¡Œé‡**: 4ç­–ç•¥ Ã— 3è§„æ¨¡ Ã— 10ç§å­ = 120æ¬¡è¿è¡Œ

#### Step 4: ç»Ÿè®¡åˆ†æ

**åˆ›å»º** `scripts/analyze_init_strategies.py`:

```python
import json
import numpy as np
import pandas as pd
from pathlib import Path
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_init_strategies(results_dir: str = "results/week1"):
    """åˆ†æä¸åŒåˆå§‹åŒ–ç­–ç•¥çš„æ•ˆæœ"""

    # æ”¶é›†æ•°æ®
    data = []
    for file in Path(results_dir).glob("init_*.json"):
        parts = file.stem.split("_")
        strategy = parts[1]
        scenario = parts[2]

        with open(file) as f:
            result = json.load(f)

        data.append({
            "strategy": strategy,
            "scenario": scenario,
            "improvement": result["improvement_ratio"],
            "runtime": result["runtime"],
            "iterations": result["iterations"]
        })

    df = pd.DataFrame(data)

    # åˆ†è§„æ¨¡åˆ†æ
    print("=" * 80)
    print("Q-tableåˆå§‹åŒ–ç­–ç•¥å¯¹æ¯”åˆ†æ")
    print("=" * 80)

    for scenario in ["small", "medium", "large"]:
        print(f"\n{'='*80}")
        print(f"{scenario.upper()} è§„æ¨¡")
        print(f"{'='*80}")

        scenario_df = df[df["scenario"] == scenario]

        # åˆ†ç­–ç•¥ç»Ÿè®¡
        summary = scenario_df.groupby("strategy")["improvement"].agg([
            ("å‡å€¼", "mean"),
            ("æ ‡å‡†å·®", "std"),
            ("æœ€å°å€¼", "min"),
            ("æœ€å¤§å€¼", "max"),
            ("å˜å¼‚ç³»æ•°", lambda x: x.std() / x.mean())
        ])

        print(summary.to_string())

        # ç»Ÿè®¡æ£€éªŒï¼šä¸é›¶åˆå§‹åŒ–å¯¹æ¯”
        zero_data = scenario_df[scenario_df["strategy"] == "zero"]["improvement"]

        for strategy in ["uniform", "action_specific", "state_specific"]:
            strategy_data = scenario_df[scenario_df["strategy"] == strategy]["improvement"]

            # Wilcoxon signed-rank test (é…å¯¹æ ·æœ¬)
            statistic, p_value = stats.wilcoxon(zero_data, strategy_data)

            # Cohen's d (æ•ˆåº”é‡)
            mean_diff = strategy_data.mean() - zero_data.mean()
            pooled_std = np.sqrt((zero_data.std()**2 + strategy_data.std()**2) / 2)
            cohens_d = mean_diff / pooled_std

            print(f"\n{strategy} vs zero:")
            print(f"  å‡å€¼å·®å¼‚: {mean_diff:+.2%}")
            print(f"  på€¼: {p_value:.4f} {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'}")
            print(f"  Cohen's d: {cohens_d:.3f} ({'å¤§' if abs(cohens_d) > 0.8 else 'ä¸­' if abs(cohens_d) > 0.5 else 'å°'}æ•ˆåº”)")

    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    for idx, scenario in enumerate(["small", "medium", "large"]):
        scenario_df = df[df["scenario"] == scenario]

        sns.boxplot(
            data=scenario_df,
            x="strategy",
            y="improvement",
            ax=axes[idx]
        )

        axes[idx].set_title(f"{scenario.upper()} Scale")
        axes[idx].set_xlabel("Initialization Strategy")
        axes[idx].set_ylabel("Improvement Ratio")
        axes[idx].tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/init_strategies_comparison.png", dpi=300)
    print(f"\nå›¾è¡¨å·²ä¿å­˜è‡³: {results_dir}/init_strategies_comparison.png")

    # æ¨èç­–ç•¥
    print("\n" + "="*80)
    print("æ¨èç­–ç•¥")
    print("="*80)

    for scenario in ["small", "medium", "large"]:
        scenario_df = df[df["scenario"] == scenario]
        best_strategy = scenario_df.groupby("strategy")["improvement"].mean().idxmax()
        best_value = scenario_df.groupby("strategy")["improvement"].mean().max()

        print(f"{scenario.upper()}: {best_strategy} (å‡å€¼æ”¹è¿›ç‡: {best_value:.2%})")

if __name__ == "__main__":
    analyze_init_strategies()
```

**è¿è¡Œåˆ†æ**:
```bash
python scripts/analyze_init_strategies.py
```

---

### ğŸ“Š Week 1 é¢„æœŸæˆæœ

**å®éªŒæ•°æ®**:
- åŸºçº¿æ•°æ®ï¼š30æ¬¡è¿è¡Œ
- åˆå§‹åŒ–å®éªŒï¼š120æ¬¡è¿è¡Œ
- æ€»è®¡ï¼š150æ¬¡è¿è¡Œ

**é¢„æœŸå‘ç°**:
1. **Uniform bias (50.0)** åœ¨æ‰€æœ‰è§„æ¨¡ä¸Šè¡¨ç°æœ€ç¨³å®š
2. **Action-specific** åœ¨å¤§è§„æ¨¡ä¸Šå¯èƒ½æœ€ä¼˜ï¼ˆåˆ©ç”¨matheuristicç®—å­ï¼‰
3. **State-specific** å¯èƒ½æ–¹å·®è¾ƒå¤§ï¼ˆä¾èµ–çŠ¶æ€è½¬ç§»ï¼‰
4. å¤§è§„æ¨¡æ”¹è¿›æœ€æ˜æ˜¾ï¼šé¢„æœŸä» 7% æå‡åˆ° 12-15%

**å¯äº¤ä»˜æˆæœ**:
- âœ… `results/week1/baseline_summary.json`
- âœ… `results/week1/init_strategies_comparison.png`
- âœ… `docs/experiments/week1_q_init_analysis.md`ï¼ˆè¯¦ç»†æŠ¥å‘Šï¼‰
- âœ… ä»£ç æ›´æ–°ï¼š`src/planner/q_learning_init.py`ï¼Œ`src/planner/q_learning.py`

**å†³ç­–**:
- é€‰æ‹©æœ€ä¼˜åˆå§‹åŒ–ç­–ç•¥ç”¨äºåç»­å®éªŒ
- å»ºè®®ï¼š**Uniform(50.0)** æˆ– **Action-specific**

---

## Week 2: Epsilonç­–ç•¥åˆ†æï¼ˆé—®é¢˜3åˆæ­¥ï¼‰

### ğŸ¯ æœ¬å‘¨ç›®æ ‡
1. åˆ†æå½“å‰epsilonç­–ç•¥çš„é—®é¢˜
2. æµ‹è¯•3ç§epsiloné…ç½®
3. è®¾è®¡è§„æ¨¡è‡ªé€‚åº”epsilonå‡½æ•°

### ğŸ“‹ é—®é¢˜3ï¼šEpsilonå›ºå®šä¸º0.12

**å½“å‰ä»£ç é—®é¢˜** (`src/config/defaults.py`):
```python
class QLearningParams:
    initial_epsilon: float = 0.12  # å¤ªä½ï¼
    epsilon_decay: float = 0.995
    epsilon_min: float = 0.01
```

**é—®é¢˜åˆ†æ**:
- æ‰€æœ‰è§„æ¨¡ä½¿ç”¨ç›¸åŒçš„ä½epsilon (0.12)
- å¤§è§„æ¨¡é—®é¢˜éœ€è¦æ›´å¤šæ¢ç´¢ï¼Œä½†ç”¨äº†ç›¸åŒé…ç½®
- å¯¼è‡´å¤§è§„æ¨¡å®ä¾‹æ¢ç´¢ä¸è¶³ï¼Œæ€§èƒ½ä¸‹é™

---

### ğŸ“… Day 1-2: Epsilonå½±å“åˆ†æ

**ä»»åŠ¡**: åˆ†æepsilonå¯¹æ€§èƒ½çš„å½±å“

**åˆ›å»ºåˆ†æè„šæœ¬** `scripts/week2_epsilon_analysis.py`:

```python
"""åˆ†æepsilonå‚æ•°å¯¹Q-learningæ€§èƒ½çš„å½±å“"""
import json
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def analyze_epsilon_impact():
    """åˆ†æepsilonç­–ç•¥çš„å½±å“"""

    # è¯»å–Week 1çš„åŸºçº¿æ•°æ®
    results_dir = Path("results/week1")

    data = {"small": [], "medium": [], "large": []}

    for file in results_dir.glob("baseline_*.json"):
        with open(file) as f:
            result = json.load(f)
            scale = result["scenario"]

            # æå–Q-learningè®­ç»ƒæ›²çº¿ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if "q_learning_stats" in result:
                epsilon_history = result["q_learning_stats"]["epsilon_history"]
                improvement = result["improvement_ratio"]

                data[scale].append({
                    "epsilon_history": epsilon_history,
                    "improvement": improvement
                })

    # åˆ†æï¼šepsilonè¡°å‡ä¸æ€§èƒ½çš„å…³ç³»
    print("=" * 60)
    print("Epsilonç­–ç•¥å½±å“åˆ†æ")
    print("=" * 60)

    for scale, runs in data.items():
        if not runs:
            continue

        print(f"\n{scale.upper()} è§„æ¨¡:")

        # è®¡ç®—å¹³å‡epsilonè¡°å‡æ›²çº¿
        epsilon_curves = [run["epsilon_history"] for run in runs]
        improvements = [run["improvement"] for run in runs]

        avg_epsilon = np.mean(epsilon_curves, axis=0)
        final_epsilon = [curve[-1] for curve in epsilon_curves]

        print(f"  åˆå§‹epsilon: 0.12")
        print(f"  æœ€ç»ˆepsilon (å¹³å‡): {np.mean(final_epsilon):.4f}")
        print(f"  å¹³å‡æ”¹è¿›ç‡: {np.mean(improvements):.2%}")

        # åˆ†æï¼šé«˜æ€§èƒ½è¿è¡Œçš„epsilonç‰¹å¾
        high_perf = [run for run in runs if run["improvement"] > np.median(improvements)]
        low_perf = [run for run in runs if run["improvement"] <= np.median(improvements)]

        if high_perf and low_perf:
            high_eps = np.mean([run["epsilon_history"] for run in high_perf], axis=0)
            low_eps = np.mean([run["epsilon_history"] for run in low_perf], axis=0)

            print(f"  é«˜æ€§èƒ½è¿è¡Œçš„å¹³å‡epsilonæ›²çº¿ä¸ä½æ€§èƒ½çš„å·®å¼‚:")
            print(f"    å‰æœŸå·®å¼‚ (iter 1-100): {np.mean(high_eps[:100] - low_eps[:100]):.4f}")
            print(f"    åæœŸå·®å¼‚ (iter -100:-1): {np.mean(high_eps[-100:] - low_eps[-100:]):.4f}")

if __name__ == "__main__":
    analyze_epsilon_impact()
```

---

### ğŸ“… Day 3-6: Epsiloné…ç½®å®éªŒ

**ä»»åŠ¡**: æµ‹è¯•3ç§epsiloné…ç½®

#### Epsiloné…ç½®è®¾è®¡

| é…ç½®å | åˆå§‹Îµ | è¡°å‡ç‡ | æœ€å°Îµ | é€‚ç”¨åœºæ™¯ | ç†è®ºä¾æ® |
|--------|-------|--------|-------|---------|---------|
| **Current** | 0.12 | 0.995 | 0.01 | å°è§„æ¨¡ï¼ˆå½“å‰ï¼‰ | å¿«é€Ÿæ”¶æ•› |
| **High-Exploration** | 0.50 | 0.995 | 0.05 | ä¸­è§„æ¨¡ | å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ |
| **Adaptive** | f(scale) | 0.998 | 0.02 | æ‰€æœ‰è§„æ¨¡ | è§„æ¨¡è‡ªé€‚åº” |

**è‡ªé€‚åº”epsilonå‡½æ•°è®¾è®¡**:

```python
def compute_adaptive_epsilon(num_requests: int) -> dict:
    """æ ¹æ®é—®é¢˜è§„æ¨¡è®¡ç®—è‡ªé€‚åº”epsilonå‚æ•°

    Args:
        num_requests: è¯·æ±‚æ•°é‡

    Returns:
        epsilonå‚æ•°å­—å…¸
    """

    if num_requests <= 12:  # å°è§„æ¨¡
        return {
            "initial_epsilon": 0.30,
            "epsilon_decay": 0.995,
            "epsilon_min": 0.01
        }
    elif num_requests <= 30:  # ä¸­è§„æ¨¡
        return {
            "initial_epsilon": 0.50,
            "epsilon_decay": 0.997,
            "epsilon_min": 0.02
        }
    else:  # å¤§è§„æ¨¡
        return {
            "initial_epsilon": 0.70,  # é«˜æ¢ç´¢ç‡
            "epsilon_decay": 0.998,   # æ…¢è¡°å‡
            "epsilon_min": 0.03       # ä¿æŒä¸€å®šæ¢ç´¢
        }
```

**ç†è®ºä¾æ®**:
- **å°è§„æ¨¡** (â‰¤12ä¸ªè¯·æ±‚): æœç´¢ç©ºé—´å°ï¼Œå¿«é€Ÿæ”¶æ•›å³å¯
- **ä¸­è§„æ¨¡** (13-30ä¸ªè¯·æ±‚): éœ€è¦å¹³è¡¡ï¼Œä¸­ç­‰æ¢ç´¢ç‡
- **å¤§è§„æ¨¡** (>30ä¸ªè¯·æ±‚): æœç´¢ç©ºé—´å·¨å¤§ï¼Œéœ€è¦å……åˆ†æ¢ç´¢

#### å®éªŒè„šæœ¬

**åˆ›å»º** `scripts/week2_epsilon_experiments.sh`:

```bash
#!/bin/bash
# Week 2: Epsilonç­–ç•¥å®éªŒ

SEEDS=(2025 2026 2027 2028 2029 2030 2031 2032 2033 2034)
SCENARIOS=("small" "medium" "large")
EPSILON_CONFIGS=("current" "high_exploration" "adaptive")

for config in "${EPSILON_CONFIGS[@]}"; do
    for scenario in "${SCENARIOS[@]}"; do
        for seed in "${SEEDS[@]}"; do
            echo "Running ${config} on ${scenario} with seed ${seed}..."
            python scripts/run_alns_preset.py \
                --scenario ${scenario} \
                --solver q_learning \
                --epsilon_config ${config} \
                --init_strategy uniform \
                --seed ${seed} \
                --output results/week2/epsilon_${config}_${scenario}_seed${seed}.json
        done
    done
done

echo "Epsilon experiments complete!"
```

**è¿è¡Œé‡**: 3é…ç½® Ã— 3è§„æ¨¡ Ã— 10ç§å­ = 90æ¬¡è¿è¡Œ

---

### ğŸ“… Day 7: ç»Ÿè®¡åˆ†æä¸ç­–ç•¥é€‰æ‹©

**åˆ†æè„šæœ¬** `scripts/analyze_epsilon_strategies.py`:

```python
import json
import numpy as np
import pandas as pd
from pathlib import Path
from scipy import stats
import matplotlib.pyplot as plt

def analyze_epsilon_strategies(results_dir: str = "results/week2"):
    """åˆ†æä¸åŒepsilonç­–ç•¥çš„æ•ˆæœ"""

    # æ”¶é›†æ•°æ®
    data = []
    for file in Path(results_dir).glob("epsilon_*.json"):
        parts = file.stem.split("_")
        config = parts[1]
        scenario = parts[2]

        with open(file) as f:
            result = json.load(f)

        data.append({
            "config": config,
            "scenario": scenario,
            "improvement": result["improvement_ratio"],
            "final_epsilon": result.get("final_epsilon", 0.01),
            "exploration_ratio": result.get("exploration_ratio", 0.0)
        })

    df = pd.DataFrame(data)

    # å¯¹æ¯”åˆ†æ
    print("=" * 80)
    print("Epsilonç­–ç•¥å¯¹æ¯”åˆ†æ")
    print("=" * 80)

    for scenario in ["small", "medium", "large"]:
        print(f"\n{scenario.upper()} è§„æ¨¡:")

        scenario_df = df[df["scenario"] == scenario]

        # ç»Ÿè®¡æ‘˜è¦
        summary = scenario_df.groupby("config").agg({
            "improvement": ["mean", "std", "min", "max"],
            "exploration_ratio": "mean"
        })

        print(summary.to_string())

        # ä¸currentå¯¹æ¯”
        current_data = scenario_df[scenario_df["config"] == "current"]["improvement"]

        for config in ["high_exploration", "adaptive"]:
            config_data = scenario_df[scenario_df["config"] == config]["improvement"]

            _, p_value = stats.wilcoxon(current_data, config_data)
            mean_diff = config_data.mean() - current_data.mean()

            print(f"\n  {config} vs current:")
            print(f"    æ”¹è¿›: {mean_diff:+.2%}")
            print(f"    på€¼: {p_value:.4f}")

    # æ¨è
    print("\n" + "="*80)
    print("æ¨èé…ç½®")
    print("="*80)

    recommendations = {}
    for scenario in ["small", "medium", "large"]:
        scenario_df = df[df["scenario"] == scenario]
        best_config = scenario_df.groupby("config")["improvement"].mean().idxmax()
        recommendations[scenario] = best_config

        print(f"{scenario}: {best_config}")

    return recommendations

if __name__ == "__main__":
    recommendations = analyze_epsilon_strategies()

    # ä¿å­˜æ¨èé…ç½®
    with open("results/week2/epsilon_recommendations.json", "w") as f:
        json.dump(recommendations, f, indent=2)
```

---

### ğŸ“Š Week 2 é¢„æœŸæˆæœ

**å®éªŒæ•°æ®**:
- Epsiloné…ç½®å®éªŒï¼š90æ¬¡è¿è¡Œ
- ä½¿ç”¨Week 1é€‰å®šçš„æœ€ä¼˜åˆå§‹åŒ–ç­–ç•¥

**é¢„æœŸå‘ç°**:
1. **Adaptive epsilon** åœ¨å¤§è§„æ¨¡ä¸Šæ˜¾è‘—ä¼˜äºå›ºå®šepsilon
2. å¤§è§„æ¨¡æ”¹è¿›ï¼šä» 12-15% (Week 1) æå‡åˆ° 18-20%
3. ç§å­æ–¹å·®è¿›ä¸€æ­¥é™ä½

**å¯äº¤ä»˜æˆæœ**:
- âœ… `results/week2/epsilon_recommendations.json`
- âœ… `docs/experiments/week2_epsilon_analysis.md`
- âœ… è‡ªé€‚åº”epsilonå‡½æ•°å®ç°

**å†³ç­–è¦ç‚¹**:
- ç¡®å®šæ¯ä¸ªè§„æ¨¡çš„æœ€ä¼˜epsiloné…ç½®
- å‡†å¤‡é›†æˆåˆ°ScaleAwareQLearningAgentï¼ˆWeek 6ï¼‰

---

## Week 3-4: ä¸ƒçŠ¶æ€ç©ºé—´è®¾è®¡ä¸å®ç°ï¼ˆé—®é¢˜2ï¼‰

### ğŸ¯ æœ¬å‘¨ç›®æ ‡
1. è®¾è®¡å¹¶å®ç°7çŠ¶æ€MDP
2. æ›¿æ¢å½“å‰çš„3çŠ¶æ€ç©ºé—´
3. é›†æˆåˆ°ALNSæ¡†æ¶
4. éªŒè¯çŠ¶æ€è½¬ç§»é€»è¾‘

### ğŸ“‹ é—®é¢˜2ï¼šçŠ¶æ€ç©ºé—´åªæœ‰3ä¸ªçŠ¶æ€

**å½“å‰ä»£ç é—®é¢˜** (`src/planner/q_learning.py` + `src/planner/alns_matheuristic.py`):

```python
# åªæœ‰3ä¸ªçŠ¶æ€
states = ("explore", "stuck", "deep_stuck")

# çŠ¶æ€è½¬ç§»é€»è¾‘è¿‡äºç®€å•ï¼ˆåªçœ‹stagnationï¼‰
if stagnation < 160:
    state = "explore"
elif stagnation < 560:
    state = "stuck"
else:
    state = "deep_stuck"
```

**é—®é¢˜åˆ†æ**:
- åªè€ƒè™‘åœæ»è®¡æ•°å™¨ï¼ˆstagnationï¼‰
- æ— æ³•åŒºåˆ†ä¼˜åŒ–è¿‡ç¨‹çš„å…¶ä»–å…³é”®ç‰¹å¾ï¼ˆæ—¶é—´ã€è´¨é‡ã€è¶‹åŠ¿ï¼‰
- çŠ¶æ€ç©ºé—´å¤ªç²—ç³™ï¼ŒQ-learningå­¦ä¸åˆ°ç»†ç²’åº¦ç­–ç•¥

---

### ğŸ“… Week 3, Day 1-2: è®¾è®¡ä¸ƒçŠ¶æ€ç©ºé—´

**ä»»åŠ¡**: è®¾è®¡æ–°çš„7çŠ¶æ€MDP

#### çŠ¶æ€ç©ºé—´è®¾è®¡

**æ–°7çŠ¶æ€å®šä¹‰**:

| çŠ¶æ€ | è‹±æ–‡å | è§¦å‘æ¡ä»¶ | ç­–ç•¥ç›®æ ‡ |
|------|--------|---------|---------|
| 1 | `early_explore` | æ—¶é—´å‰©ä½™>80% | å¹¿æ³›æ¢ç´¢ï¼Œå°è¯•å„ç§ç®—å­ |
| 2 | `active_improve` | æŒç»­æ”¹è¿›ä¸­ + åœæ»<é˜ˆå€¼1 | ä¿æŒå½“å‰ç­–ç•¥ï¼ŒæŒç»­æ”¹è¿› |
| 3 | `slow_progress` | æ”¹è¿›å˜æ…¢ + åœæ»<é˜ˆå€¼2 | åŠ å¤§ç ´ååŠ›åº¦ï¼Œå¯»æ‰¾çªç ´ |
| 4 | `plateau` | åœæ»â‰¥é˜ˆå€¼2 + æ—¶é—´å‰©ä½™>30% | å°è¯•matheuristicç®—å­ |
| 5 | `intensive_search` | åœæ»â‰¥é˜ˆå€¼2 + æ—¶é—´å‰©ä½™â‰¤30% | æ·±åº¦æœç´¢ï¼Œæ¿€è¿›ç­–ç•¥ |
| 6 | `final_polish` | æ—¶é—´å‰©ä½™<15% + åœæ»<é˜ˆå€¼3 | å±€éƒ¨ä¼˜åŒ–ï¼Œå¿«é€Ÿä¿®å¤ |
| 7 | `emergency` | åœæ»â‰¥é˜ˆå€¼3 | æœ€æ¿€è¿›ç­–ç•¥ï¼Œæ‰“ç ´åƒµå±€ |

**è§„æ¨¡è‡ªé€‚åº”é˜ˆå€¼**:

| è§„æ¨¡ | stag_1 | stag_2 | stag_3 | max_iter |
|------|--------|--------|--------|----------|
| Small | 80 | 200 | 400 | 1000 |
| Medium | 120 | 300 | 600 | 2000 |
| Large | 160 | 400 | 800 | 4000 |

**çŠ¶æ€ç‰¹å¾**:

```python
@dataclass
class StateFeatures:
    """çŠ¶æ€åˆ†ç±»æ‰€éœ€çš„ç‰¹å¾"""
    stagnation: int          # åœæ»è®¡æ•°å™¨ï¼ˆè‡ªä¸Šæ¬¡æ”¹è¿›çš„è¿­ä»£æ•°ï¼‰
    solution_quality: float  # å½“å‰è§£è´¨é‡ = current_cost / initial_cost
    time_remaining: float    # å‰©ä½™æ—¶é—´æ¯”ä¾‹ = 1 - (iter / max_iter)
    improvement_trend: str   # æ”¹è¿›è¶‹åŠ¿: "improving", "stable", "degrading"
```

**æ”¹è¿›è¶‹åŠ¿åˆ¤å®š**:

```python
def classify_improvement_trend(recent_improvements: deque) -> str:
    """æ ¹æ®æœ€è¿‘5æ¬¡è¿­ä»£çš„æ”¹è¿›åˆ¤æ–­è¶‹åŠ¿

    Args:
        recent_improvements: æœ€è¿‘5æ¬¡æ”¹è¿›å€¼çš„é˜Ÿåˆ—

    Returns:
        "improving": æŒç»­æ”¹è¿›
        "stable": ç¨³å®š
        "degrading": æ¶åŒ–
    """
    if len(recent_improvements) < 3:
        return "stable"

    # è®¡ç®—ç§»åŠ¨å¹³å‡æ–œç‡
    x = np.arange(len(recent_improvements))
    y = np.array(recent_improvements)

    slope, _ = np.polyfit(x, y, 1)

    if slope > 0.1:  # æŒç»­æ”¹è¿›
        return "improving"
    elif slope < -0.1:  # æ¶åŒ–
        return "degrading"
    else:
        return "stable"
```

---

### ğŸ“… Week 3, Day 3-4: å®ç°çŠ¶æ€åˆ†ç±»å™¨

**åˆ›å»ºæ–°æ¨¡å—** `src/planner/state_classifier.py`:

```python
"""Seven-state classifier for Scale-Aware Q-Learning"""
from dataclasses import dataclass
from typing import Literal
import numpy as np

StateLabel = Literal[
    "early_explore",
    "active_improve",
    "slow_progress",
    "plateau",
    "intensive_search",
    "final_polish",
    "emergency"
]

@dataclass
class StateFeatures:
    """çŠ¶æ€åˆ†ç±»æ‰€éœ€çš„ç‰¹å¾"""
    stagnation: int          # åœæ»è®¡æ•°å™¨
    solution_quality: float  # å½“å‰è§£è´¨é‡/åˆå§‹è§£
    time_remaining: float    # å‰©ä½™æ—¶é—´æ¯”ä¾‹ [0, 1]
    improvement_trend: str   # "improving", "stable", "degrading"

class SevenStateSpace:
    """ä¸ƒçŠ¶æ€ç©ºé—´åˆ†ç±»å™¨"""

    # çŠ¶æ€å®šä¹‰
    STATES = (
        "early_explore",
        "active_improve",
        "slow_progress",
        "plateau",
        "intensive_search",
        "final_polish",
        "emergency"
    )

    # è§„æ¨¡ç›¸å…³é˜ˆå€¼
    SCALE_THRESHOLDS = {
        "small": {
            "stag_1": 80,
            "stag_2": 200,
            "stag_3": 400,
        },
        "medium": {
            "stag_1": 120,
            "stag_2": 300,
            "stag_3": 600,
        },
        "large": {
            "stag_1": 160,
            "stag_2": 400,
            "stag_3": 800,
        }
    }

    @classmethod
    def classify_state(
        cls,
        features: StateFeatures,
        scale: str = "medium"
    ) -> StateLabel:
        """æ ¹æ®ç‰¹å¾åˆ†ç±»çŠ¶æ€

        Args:
            features: çŠ¶æ€ç‰¹å¾
            scale: é—®é¢˜è§„æ¨¡ ("small", "medium", "large")

        Returns:
            çŠ¶æ€æ ‡ç­¾
        """

        thresholds = cls.SCALE_THRESHOLDS.get(scale, cls.SCALE_THRESHOLDS["medium"])

        stag = features.stagnation
        time_left = features.time_remaining
        trend = features.improvement_trend

        # è§„åˆ™1: æ—©æœŸæ¢ç´¢ï¼ˆæ—¶é—´å……è¶³ï¼‰
        if time_left > 0.80:
            return "early_explore"

        # è§„åˆ™2: æŒç»­æ”¹è¿›ä¸­
        if trend == "improving" and stag < thresholds["stag_1"]:
            return "active_improve"

        # è§„åˆ™3: æ”¹è¿›å˜æ…¢
        if trend == "stable" and stag < thresholds["stag_2"]:
            return "slow_progress"

        # è§„åˆ™4: å¹³å°æœŸï¼ˆæœ‰æ—¶é—´ï¼‰
        if stag >= thresholds["stag_2"] and time_left > 0.30:
            return "plateau"

        # è§„åˆ™5: æ”¶å°¾ä¼˜åŒ–
        if time_left < 0.15 and stag < thresholds["stag_3"]:
            return "final_polish"

        # è§„åˆ™6: æ·±åº¦æœç´¢
        if stag >= thresholds["stag_2"] and stag < thresholds["stag_3"]:
            return "intensive_search"

        # è§„åˆ™7: ç´§æ€¥çŠ¶æ€
        return "emergency"

    @classmethod
    def get_state_description(cls, state: StateLabel) -> str:
        """è·å–çŠ¶æ€æè¿°"""
        descriptions = {
            "early_explore": "æ—©æœŸæ¢ç´¢é˜¶æ®µï¼Œå¹¿æ³›å°è¯•å„ç§ç®—å­",
            "active_improve": "æŒç»­æ”¹è¿›ä¸­ï¼Œä¿æŒå½“å‰ç­–ç•¥",
            "slow_progress": "æ”¹è¿›å˜æ…¢ï¼Œéœ€è¦åŠ å¤§ç ´ååŠ›åº¦",
            "plateau": "å¹³å°æœŸï¼Œå°è¯•matheuristicç®—å­çªç ´",
            "intensive_search": "æ·±åº¦æœç´¢ï¼Œé‡‡ç”¨æ¿€è¿›ç­–ç•¥",
            "final_polish": "æ”¶å°¾ä¼˜åŒ–ï¼Œå¿«é€Ÿä¿®å¤",
            "emergency": "ç´§æ€¥çŠ¶æ€ï¼Œæœ€æ¿€è¿›ç­–ç•¥æ‰“ç ´åƒµå±€"
        }
        return descriptions.get(state, "æœªçŸ¥çŠ¶æ€")
```

**å•å…ƒæµ‹è¯•** `tests/test_state_classifier.py`:

```python
"""Tests for seven-state classifier"""
import pytest
from planner.state_classifier import SevenStateSpace, StateFeatures

class TestSevenStateSpace:
    """æµ‹è¯•ä¸ƒçŠ¶æ€åˆ†ç±»å™¨"""

    def test_early_explore(self):
        """æµ‹è¯•æ—©æœŸæ¢ç´¢çŠ¶æ€"""
        features = StateFeatures(
            stagnation=50,
            solution_quality=0.95,
            time_remaining=0.85,
            improvement_trend="improving"
        )

        state = SevenStateSpace.classify_state(features, scale="medium")
        assert state == "early_explore"

    def test_active_improve(self):
        """æµ‹è¯•æŒç»­æ”¹è¿›çŠ¶æ€"""
        features = StateFeatures(
            stagnation=100,
            solution_quality=0.80,
            time_remaining=0.60,
            improvement_trend="improving"
        )

        state = SevenStateSpace.classify_state(features, scale="medium")
        assert state == "active_improve"

    def test_plateau(self):
        """æµ‹è¯•å¹³å°æœŸçŠ¶æ€"""
        features = StateFeatures(
            stagnation=350,
            solution_quality=0.75,
            time_remaining=0.40,
            improvement_trend="stable"
        )

        state = SevenStateSpace.classify_state(features, scale="medium")
        assert state == "plateau"

    def test_emergency(self):
        """æµ‹è¯•ç´§æ€¥çŠ¶æ€"""
        features = StateFeatures(
            stagnation=650,
            solution_quality=0.70,
            time_remaining=0.10,
            improvement_trend="degrading"
        )

        state = SevenStateSpace.classify_state(features, scale="medium")
        assert state == "emergency"

    def test_scale_adaptation(self):
        """æµ‹è¯•è§„æ¨¡è‡ªé€‚åº”"""
        features = StateFeatures(
            stagnation=150,
            solution_quality=0.80,
            time_remaining=0.50,
            improvement_trend="stable"
        )

        # å°è§„æ¨¡ï¼š150 > stag_2(200) -> slow_progress
        state_small = SevenStateSpace.classify_state(features, scale="small")

        # å¤§è§„æ¨¡ï¼š150 < stag_2(400) -> slow_progress
        state_large = SevenStateSpace.classify_state(features, scale="large")

        # éƒ½åº”è¯¥æ˜¯slow_progressï¼ˆé€»è¾‘éœ€è°ƒæ•´ï¼‰
        assert state_small in SevenStateSpace.STATES
        assert state_large in SevenStateSpace.STATES

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

**è¿è¡Œæµ‹è¯•**:
```bash
pytest tests/test_state_classifier.py -v
```

---

### ğŸ“… Week 3, Day 5-7 + Week 4, Day 1-3: æ›´æ–°Q-learning Agent

**ä¿®æ”¹æ–‡ä»¶** `src/planner/q_learning.py`:

```python
"""Scale-Aware Q-Learning Agent"""
from planner.state_classifier import SevenStateSpace, StateFeatures
from planner.q_learning_init import QInitStrategy, INIT_STRATEGIES
from typing import Optional

class ScaleAwareQLearningAgent(QLearningOperatorAgent):
    """è§„æ¨¡è‡ªé€‚åº”Q-learningä»£ç†ï¼ˆä½¿ç”¨7çŠ¶æ€ç©ºé—´ï¼‰"""

    def __init__(
        self,
        destroy_operators: Iterable[str],
        repair_operators: Sequence[str],
        params: QLearningParams,
        scale: str,  # NEW: "small", "medium", "large"
        *,
        state_classifier: Optional[SevenStateSpace] = None,
        init_strategy: QInitStrategy = QInitStrategy.UNIFORM,
    ):
        """
        Args:
            destroy_operators: ç ´åç®—å­åˆ—è¡¨
            repair_operators: ä¿®å¤ç®—å­åˆ—è¡¨
            params: Q-learningå‚æ•°
            scale: é—®é¢˜è§„æ¨¡
            state_classifier: çŠ¶æ€åˆ†ç±»å™¨ï¼ˆå¯é€‰ï¼‰
            init_strategy: Q-tableåˆå§‹åŒ–ç­–ç•¥
        """

        self.scale = scale
        self.state_classifier = state_classifier or SevenStateSpace()

        # ä½¿ç”¨7çŠ¶æ€ç©ºé—´
        state_labels = SevenStateSpace.STATES

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            destroy_operators,
            repair_operators,
            params,
            state_labels=state_labels,
            init_strategy=init_strategy,
        )

        # è®¾ç½®è§„æ¨¡è‡ªé€‚åº”epsilon
        self.set_epsilon(self._compute_scale_epsilon())

    def _compute_scale_epsilon(self) -> float:
        """è®¡ç®—è§„æ¨¡è‡ªé€‚åº”epsilon"""
        scale_map = {
            "small": 0.30,
            "medium": 0.50,
            "large": 0.70
        }
        return scale_map.get(self.scale, 0.50)

    def classify_state(self, features: StateFeatures) -> str:
        """åˆ†ç±»å½“å‰çŠ¶æ€

        Args:
            features: çŠ¶æ€ç‰¹å¾

        Returns:
            çŠ¶æ€æ ‡ç­¾
        """
        return self.state_classifier.classify_state(features, self.scale)
```

---

### ğŸ“… Week 4, Day 4-7: é›†æˆåˆ°ALNSä¸»å¾ªç¯

**ä¿®æ”¹æ–‡ä»¶** `src/planner/alns_matheuristic.py`:

```python
"""ALNS with Scale-Aware Q-Learning"""
from collections import deque
import numpy as np
from planner.state_classifier import StateFeatures

class MatheuristicALNS:
    """Matheuristic ALNS with Scale-Aware Q-Learning"""

    def optimize(self) -> Solution:
        """ä¸»ä¼˜åŒ–å¾ªç¯"""

        # åˆå§‹åŒ–
        current = self.construct_initial_solution()
        best = current

        iteration = 0
        stagnation_counter = 0
        recent_improvements = deque(maxlen=5)  # NEW: è¿½è¸ªæœ€è¿‘5æ¬¡æ”¹è¿›

        # ç¡®å®šé—®é¢˜è§„æ¨¡
        num_requests = len(self.scenario.requests)
        scale = self._determine_scale(num_requests)

        while iteration < self.max_iterations:
            # ========== æ„å»ºçŠ¶æ€ç‰¹å¾ ==========

            # è®¡ç®—æ”¹è¿›è¶‹åŠ¿
            improvement_trend = self._classify_trend(recent_improvements)

            # æ„å»ºçŠ¶æ€ç‰¹å¾
            features = StateFeatures(
                stagnation=stagnation_counter,
                solution_quality=current.cost / self._initial_cost,
                time_remaining=1.0 - (iteration / self.max_iterations),
                improvement_trend=improvement_trend
            )

            # ä»Q-agentè·å–çŠ¶æ€
            if self.adaptation_mode == "q_learning":
                state = self.q_agent.classify_state(features)

            # ========== ALNSè¿­ä»£ ==========

            # é€‰æ‹©ç®—å­
            if self.adaptation_mode == "q_learning":
                destroy_op, repair_op = self.q_agent.select_action(state)
            else:
                # è½®ç›˜èµŒé€‰æ‹©
                destroy_op, repair_op = self._roulette_select()

            # æ‰§è¡Œdestroy-repair
            destroyed = self._apply_destroy(current, destroy_op)
            candidate = self._apply_repair(destroyed, repair_op)

            # æ¥å—å‡†åˆ™
            is_accepted = self._accept(candidate, current)

            if is_accepted:
                current = candidate

                # è®¡ç®—æ”¹è¿›
                improvement = best.cost - current.cost if current.cost < best.cost else 0.0
                recent_improvements.append(improvement)

                if improvement > 0:
                    best = current
                    stagnation_counter = 0
                else:
                    stagnation_counter += 1
            else:
                stagnation_counter += 1
                recent_improvements.append(0.0)

            # æ›´æ–°Q-learning
            if self.adaptation_mode == "q_learning":
                reward = self._compute_reward(
                    improvement=improvement,
                    is_accepted=is_accepted,
                    # ... å…¶ä»–å‚æ•°
                )

                # è®¡ç®—ä¸‹ä¸€çŠ¶æ€
                next_features = StateFeatures(
                    stagnation=stagnation_counter,
                    solution_quality=current.cost / self._initial_cost,
                    time_remaining=1.0 - ((iteration + 1) / self.max_iterations),
                    improvement_trend=self._classify_trend(recent_improvements)
                )
                next_state = self.q_agent.classify_state(next_features)

                # Q-learningæ›´æ–°
                self.q_agent.update(
                    state=state,
                    action=(destroy_op, repair_op),
                    reward=reward,
                    next_state=next_state
                )

            iteration += 1

        return best

    def _determine_scale(self, num_requests: int) -> str:
        """ç¡®å®šé—®é¢˜è§„æ¨¡"""
        if num_requests <= 12:
            return "small"
        elif num_requests <= 30:
            return "medium"
        else:
            return "large"

    def _classify_trend(self, recent_improvements: deque) -> str:
        """åˆ†ç±»æ”¹è¿›è¶‹åŠ¿"""
        if len(recent_improvements) < 3:
            return "stable"

        # è®¡ç®—ç§»åŠ¨å¹³å‡æ–œç‡
        x = np.arange(len(recent_improvements))
        y = np.array(recent_improvements)

        if len(x) < 2:
            return "stable"

        slope, _ = np.polyfit(x, y, 1)

        if slope > 0.001:  # æŒç»­æ”¹è¿›
            return "improving"
        elif slope < -0.001:  # æ¶åŒ–
            return "degrading"
        else:
            return "stable"
```

---

### ğŸ“… Week 4, Day 7: éªŒè¯å®éªŒ

**åˆ›å»ºéªŒè¯è„šæœ¬** `scripts/week4_seven_state_validation.sh`:

```bash
#!/bin/bash
# Week 4: ä¸ƒçŠ¶æ€ç©ºé—´éªŒè¯å®éªŒ

SEEDS=(2025 2026 2027 2028 2029 2030 2031 2032 2033 2034)
SCENARIOS=("small" "medium" "large")

# å¯¹æ¯”3çŠ¶æ€ vs 7çŠ¶æ€
for scenario in "${SCENARIOS[@]}"; do
    for seed in "${SEEDS[@]}"; do
        # 3çŠ¶æ€ï¼ˆåŸºçº¿ï¼‰
        python scripts/run_alns_preset.py \
            --scenario ${scenario} \
            --solver q_learning \
            --state_space 3 \
            --init_strategy uniform \
            --seed ${seed} \
            --output results/week4/3state_${scenario}_seed${seed}.json

        # 7çŠ¶æ€ï¼ˆæ–°ï¼‰
        python scripts/run_alns_preset.py \
            --scenario ${scenario} \
            --solver q_learning_saql \
            --state_space 7 \
            --init_strategy uniform \
            --seed ${seed} \
            --output results/week4/7state_${scenario}_seed${seed}.json
    done
done

echo "Seven-state validation complete!"
```

**åˆ†æè„šæœ¬** `scripts/analyze_state_space_comparison.py`:

```python
"""å¯¹æ¯”3çŠ¶æ€ vs 7çŠ¶æ€ç©ºé—´"""
import json
import numpy as np
import pandas as pd
from pathlib import Path
from scipy import stats

def compare_state_spaces(results_dir: str = "results/week4"):
    """å¯¹æ¯”3çŠ¶æ€å’Œ7çŠ¶æ€ç©ºé—´çš„æ€§èƒ½"""

    # æ”¶é›†æ•°æ®
    data = []
    for file in Path(results_dir).glob("*state_*.json"):
        parts = file.stem.split("_")
        state_space = parts[0]  # "3state" or "7state"
        scenario = parts[1]

        with open(file) as f:
            result = json.load(f)

        data.append({
            "state_space": state_space,
            "scenario": scenario,
            "improvement": result["improvement_ratio"],
            "state_transitions": result.get("state_transition_count", 0)
        })

    df = pd.DataFrame(data)

    # åˆ†è§„æ¨¡å¯¹æ¯”
    print("=" * 80)
    print("3çŠ¶æ€ vs 7çŠ¶æ€ç©ºé—´å¯¹æ¯”")
    print("=" * 80)

    for scenario in ["small", "medium", "large"]:
        print(f"\n{scenario.upper()} è§„æ¨¡:")

        scenario_df = df[df["scenario"] == scenario]

        three_state = scenario_df[scenario_df["state_space"] == "3state"]["improvement"]
        seven_state = scenario_df[scenario_df["state_space"] == "7state"]["improvement"]

        # ç»Ÿè®¡é‡
        print(f"  3çŠ¶æ€: {three_state.mean():.2%} Â± {three_state.std():.2%}")
        print(f"  7çŠ¶æ€: {seven_state.mean():.2%} Â± {seven_state.std():.2%}")

        # æ”¹è¿›
        improvement = seven_state.mean() - three_state.mean()
        print(f"  æ”¹è¿›: {improvement:+.2%}")

        # ç»Ÿè®¡æ£€éªŒ
        _, p_value = stats.wilcoxon(three_state, seven_state)
        print(f"  på€¼: {p_value:.4f} {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'}")

    # çŠ¶æ€è½¬ç§»åˆ†æ
    print("\n" + "="*80)
    print("çŠ¶æ€è½¬ç§»ç»Ÿè®¡")
    print("="*80)

    seven_state_df = df[df["state_space"] == "7state"]
    for scenario in ["small", "medium", "large"]:
        scenario_data = seven_state_df[seven_state_df["scenario"] == scenario]
        avg_transitions = scenario_data["state_transitions"].mean()
        print(f"{scenario}: å¹³å‡çŠ¶æ€è½¬ç§»æ¬¡æ•° = {avg_transitions:.1f}")

if __name__ == "__main__":
    compare_state_spaces()
```

---

### ğŸ“Š Week 3-4 é¢„æœŸæˆæœ

**å®éªŒæ•°æ®**:
- 3çŠ¶æ€ vs 7çŠ¶æ€å¯¹æ¯”ï¼š60æ¬¡è¿è¡Œï¼ˆ30+30ï¼‰
- ä½¿ç”¨Week 1-2é€‰å®šçš„æœ€ä¼˜é…ç½®ï¼ˆåˆå§‹åŒ–+epsilonï¼‰

**é¢„æœŸå‘ç°**:
1. **7çŠ¶æ€åœ¨å¤§è§„æ¨¡ä¸Šæ˜¾è‘—ä¼˜äº3çŠ¶æ€**
2. çŠ¶æ€è½¬ç§»æ›´é¢‘ç¹ï¼Œç­–ç•¥æ›´çµæ´»
3. å¤§è§„æ¨¡æ”¹è¿›ï¼šä» 18-20% (Week 2) æå‡åˆ° 22-25%

**å¯äº¤ä»˜æˆæœ**:
- âœ… `src/planner/state_classifier.py` (150è¡Œ)
- âœ… `src/planner/q_learning.py` æ›´æ–° (ScaleAwareQLearningAgent)
- âœ… `src/planner/alns_matheuristic.py` æ›´æ–° (çŠ¶æ€ç‰¹å¾è¿½è¸ª)
- âœ… `tests/test_state_classifier.py` (å•å…ƒæµ‹è¯•)
- âœ… `docs/experiments/week3-4_seven_state_analysis.md`

**å…³é”®æŒ‡æ ‡**:
- å¤§è§„æ¨¡æ”¹è¿›ç‡ï¼šç›®æ ‡ â‰¥22%
- çŠ¶æ€è½¬ç§»æ¬¡æ•°ï¼šç›®æ ‡ >50æ¬¡/è¿è¡Œ
- é€šè¿‡æ‰€æœ‰å•å…ƒæµ‹è¯•

---

## Week 5: è§„æ¨¡è‡ªé€‚åº”å¥–åŠ±å½’ä¸€åŒ–ï¼ˆé—®é¢˜4ï¼‰

### ğŸ¯ æœ¬å‘¨ç›®æ ‡
1. è®¾è®¡è§„æ¨¡æ— å…³çš„å¥–åŠ±å‡½æ•°
2. æ¶ˆé™¤è·¨è§„æ¨¡å¥–åŠ±æ–¹å·®
3. A/Bæµ‹è¯•éªŒè¯æ•ˆæœ

### ğŸ“‹ é—®é¢˜4ï¼šå¥–åŠ±æœªå½’ä¸€åŒ–

**å½“å‰ä»£ç é—®é¢˜** (`src/planner/alns.py:623-696`):

```python
def _compute_q_reward(...) -> float:
    baseline_cost = self._initial_solution_cost  # è§„æ¨¡ç›¸å…³ï¼
    relative_gain = improvement / baseline_cost  # ä¸åŒè§„æ¨¡å·®å¼‚å¤§
    quality += relative_gain * params.roi_positive_scale  # 220.0
```

**é—®é¢˜åˆ†æ**:
- `baseline_cost` å˜åŒ–ï¼šå°è§„æ¨¡~35Kï¼Œå¤§è§„æ¨¡~52K
- ç›¸åŒç»å¯¹æ”¹è¿›ï¼ˆå¦‚500ï¼‰åœ¨ä¸åŒè§„æ¨¡ä¸‹å¥–åŠ±ä¸åŒ
- Qå€¼å­¦ä¹ æ··ä¹±ï¼Œè·¨è§„æ¨¡ä¸ç¨³å®š

---

### ğŸ“… Day 1-2: è®¾è®¡è§„æ¨¡è‡ªé€‚åº”å¥–åŠ±å‡½æ•°

**è®¾è®¡åŸåˆ™**:
1. **ç™¾åˆ†æ¯”å½’ä¸€åŒ–**: ä½¿ç”¨ç›¸å¯¹æ”¹è¿›è€Œéç»å¯¹å€¼
2. **è§„æ¨¡å› å­è¡¥å¿**: å¤§è§„æ¨¡é—®é¢˜æ›´éš¾ï¼Œç»™äºˆæ›´é«˜åŸºç¡€å¥–åŠ±
3. **æ—¶é—´æˆæœ¬è‡ªé€‚åº”**: ä¸åŒè§„æ¨¡å¯¹ç®—å­è€—æ—¶çš„å®¹å¿åº¦ä¸åŒ

#### æ–°å¥–åŠ±å‚æ•°è®¾è®¡

**åˆ›å»ºé…ç½®** `src/config/defaults.py`:

```python
from dataclasses import dataclass, field
from typing import Dict

@dataclass
class ScaleAwareRewardParams:
    """è§„æ¨¡è‡ªé€‚åº”å¥–åŠ±å‚æ•°"""

    # ========== åŸºç¡€å¥–åŠ±ï¼ˆè§„æ¨¡æ— å…³ï¼‰ ==========
    reward_new_best_base: float = 100.0      # å‘ç°æ–°æœ€ä¼˜è§£
    reward_improvement_base: float = 50.0    # æœ‰æ”¹è¿›
    reward_accepted_base: float = 10.0       # è§£è¢«æ¥å—
    reward_rejected: float = -5.0            # è§£è¢«æ‹’ç»

    # ========== è§„æ¨¡å› å­ï¼ˆè¡¥å¿å¤§è§„æ¨¡éš¾åº¦ï¼‰ ==========
    scale_factors: Dict[str, float] = field(default_factory=lambda: {
        "small": 1.0,    # åŸºå‡†
        "medium": 1.2,   # ä¸­ç­‰éš¾åº¦ï¼Œç•¥å¾®æå‡
        "large": 1.5     # é«˜éš¾åº¦ï¼Œæ˜¾è‘—æå‡
    })

    # ========== ROIç¼©æ”¾ï¼ˆç™¾åˆ†æ¯”æ”¾å¤§ï¼‰ ==========
    roi_scale: float = 1000.0  # æ”¾å¤§å°ç™¾åˆ†æ¯”æ”¹è¿›

    # ========== æ—¶é—´æƒ©ç½šï¼ˆè§„æ¨¡è‡ªé€‚åº”ï¼‰ ==========
    time_penalty_scale: Dict[str, float] = field(default_factory=lambda: {
        "small": 1.0,    # å°è§„æ¨¡å¯¹æ—¶é—´ä¸æ•æ„Ÿ
        "medium": 1.5,   # ä¸­ç­‰æ•æ„Ÿ
        "large": 2.0     # å¤§è§„æ¨¡å¯¹æ…¢ç®—å­æ›´ä¸¥æ ¼
    })

    # ========== æ—¶é—´æˆæœ¬é¢„æœŸï¼ˆç§’ï¼‰ ==========
    expected_time_cost: Dict[str, float] = field(default_factory=lambda: {
        "small": 0.5,
        "medium": 1.0,
        "large": 2.0
    })
```

#### æ–°å¥–åŠ±å‡½æ•°å®ç°

**ä¿®æ”¹æ–‡ä»¶** `src/planner/alns.py`:

```python
def _compute_scale_aware_reward(
    self,
    *,
    improvement: float,
    is_new_best: bool,
    is_accepted: bool,
    action_cost: float,
    repair_operator: str,
    previous_cost: float,
    scale: str,  # NEW: ä¼ å…¥è§„æ¨¡
) -> float:
    """è®¡ç®—è§„æ¨¡è‡ªé€‚åº”å¥–åŠ±

    Args:
        improvement: ç»å¯¹æ”¹è¿›å€¼ï¼ˆprevious_cost - new_costï¼‰
        is_new_best: æ˜¯å¦ä¸ºæ–°æœ€ä¼˜è§£
        is_accepted: æ˜¯å¦è¢«æ¥å—
        action_cost: ç®—å­æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
        repair_operator: ä¿®å¤ç®—å­åç§°
        previous_cost: å‰ä¸€è§£çš„æˆæœ¬
        scale: é—®é¢˜è§„æ¨¡ ("small", "medium", "large")

    Returns:
        å¥–åŠ±å€¼ï¼ˆå¯ä¸ºè´Ÿï¼‰
    """

    params = self.sa_reward_params  # ScaleAwareRewardParamså®ä¾‹
    scale_factor = params.scale_factors[scale]

    # ========== 1. è´¨é‡ç»„ä»¶ï¼ˆè§„æ¨¡æ ‡å‡†åŒ–ï¼‰ ==========
    quality = 0.0

    if is_new_best:
        quality = params.reward_new_best_base * scale_factor
    elif improvement > 0:
        quality = params.reward_improvement_base * scale_factor
    elif is_accepted:
        quality = params.reward_accepted_base * scale_factor
    else:
        quality = params.reward_rejected  # è¢«æ‹’ç»

    # ========== 2. ROIç»„ä»¶ï¼ˆç™¾åˆ†æ¯”å½’ä¸€åŒ–ï¼Œè§„æ¨¡æ— å…³ï¼‰ ==========
    if improvement > 0 and previous_cost > 0:
        # ç›¸å¯¹æ”¹è¿›ç™¾åˆ†æ¯”
        relative_improvement = improvement / previous_cost

        # æ”¾å¤§å°ç™¾åˆ†æ¯”ï¼Œä½¿å…¶å¯¹Q-learningæœ‰æ„ä¹‰
        roi_reward = relative_improvement * params.roi_scale * scale_factor

        quality += roi_reward

    # ========== 3. æ—¶é—´æƒ©ç½šï¼ˆè§„æ¨¡è‡ªé€‚åº”ï¼‰ ==========
    is_matheuristic = repair_operator in ["greedy_lp", "segments"]

    if is_matheuristic and action_cost > 0:
        # è§„æ¨¡ç›¸å…³çš„é¢„æœŸæ—¶é—´
        expected_cost = params.expected_time_cost[scale]

        # åªæœ‰å½“è€—æ—¶è¶…å‡ºé¢„æœŸæ—¶æ‰æƒ©ç½š
        if action_cost > expected_cost:
            # è®¡ç®—æ”¶ç›Šæˆæœ¬æ¯”
            benefit_ratio = improvement / (previous_cost * 0.01) if previous_cost > 0 else 0
            cost_ratio = action_cost / expected_cost

            # åªæœ‰å½“æ”¶ç›Šä¸å€¼å¾—æˆæœ¬æ—¶æ‰æƒ©ç½š
            if benefit_ratio < cost_ratio:
                penalty = (cost_ratio - benefit_ratio) * params.time_penalty_scale[scale]
                quality -= penalty * 10.0  # æƒ©ç½šç³»æ•°

    return quality
```

---

### ğŸ“… Day 3-5: å®ç°ä¸é›†æˆ

#### Step 1: æ›´æ–°ALNSä¸»å¾ªç¯

**ä¿®æ”¹** `src/planner/alns_matheuristic.py`:

```python
class MatheuristicALNS:
    """Matheuristic ALNS with Scale-Aware Rewards"""

    def __init__(self, scenario: Scenario, preset: str = "medium", seed: Optional[int] = None):
        # ... ç°æœ‰åˆå§‹åŒ– ...

        # NEW: æ·»åŠ è§„æ¨¡è‡ªé€‚åº”å¥–åŠ±å‚æ•°
        self.sa_reward_params = ScaleAwareRewardParams()

        # ç¡®å®šé—®é¢˜è§„æ¨¡
        num_requests = len(scenario.requests)
        self.scale = self._determine_scale(num_requests)

    def optimize(self) -> Solution:
        """ä¸»ä¼˜åŒ–å¾ªç¯"""

        # ... ALNSè¿­ä»£ ...

        # è®¡ç®—å¥–åŠ±ï¼ˆä½¿ç”¨æ–°å‡½æ•°ï¼‰
        if self.adaptation_mode == "q_learning":
            reward = self._compute_scale_aware_reward(
                improvement=improvement,
                is_new_best=(candidate.cost < best.cost),
                is_accepted=is_accepted,
                action_cost=repair_time,
                repair_operator=repair_op,
                previous_cost=current.cost,
                scale=self.scale  # ä¼ å…¥è§„æ¨¡
            )

            # Q-learningæ›´æ–°
            self.q_agent.update(state, action, reward, next_state)
```

#### Step 2: A/Bæµ‹è¯•å®éªŒ

**åˆ›å»ºå®éªŒè„šæœ¬** `scripts/week5_reward_normalization_test.sh`:

```bash
#!/bin/bash
# Week 5: å¥–åŠ±å½’ä¸€åŒ–A/Bæµ‹è¯•

SEEDS=(2025 2026 2027 2028 2029 2030 2031 2032 2033 2034)
SCENARIOS=("small" "medium" "large")

for scenario in "${SCENARIOS[@]}"; do
    for seed in "${SEEDS[@]}"; do
        # A: æ—§å¥–åŠ±å‡½æ•°
        python scripts/run_alns_preset.py \
            --scenario ${scenario} \
            --solver q_learning_saql \
            --reward_function original \
            --seed ${seed} \
            --output results/week5/reward_original_${scenario}_seed${seed}.json

        # B: æ–°å¥–åŠ±å‡½æ•°ï¼ˆè§„æ¨¡è‡ªé€‚åº”ï¼‰
        python scripts/run_alns_preset.py \
            --scenario ${scenario} \
            --solver q_learning_saql \
            --reward_function scale_aware \
            --seed ${seed} \
            --output results/week5/reward_scale_aware_${scenario}_seed${seed}.json
    done
done

echo "Reward normalization A/B test complete!"
```

**è¿è¡Œé‡**: 2ç§å¥–åŠ± Ã— 3è§„æ¨¡ Ã— 10ç§å­ = 60æ¬¡è¿è¡Œ

---

### ğŸ“… Day 6-7: åˆ†æä¸éªŒè¯

**åˆ†æè„šæœ¬** `scripts/analyze_reward_normalization.py`:

```python
"""åˆ†æå¥–åŠ±å½’ä¸€åŒ–çš„æ•ˆæœ"""
import json
import numpy as np
import pandas as pd
from pathlib import Path
from scipy import stats
import matplotlib.pyplot as plt

def analyze_reward_normalization(results_dir: str = "results/week5"):
    """åˆ†æå¥–åŠ±å½’ä¸€åŒ–æ•ˆæœ"""

    # æ”¶é›†æ•°æ®
    data = []
    for file in Path(results_dir).glob("reward_*.json"):
        parts = file.stem.split("_")
        reward_type = parts[1]  # "original" or "scale_aware"
        scenario = parts[2]

        with open(file) as f:
            result = json.load(f)

        data.append({
            "reward_type": reward_type,
            "scenario": scenario,
            "improvement": result["improvement_ratio"],
            "reward_variance": result.get("reward_variance", 0),
            "convergence_iter": result.get("convergence_iteration", 0)
        })

    df = pd.DataFrame(data)

    # ========== æ€§èƒ½å¯¹æ¯” ==========
    print("=" * 80)
    print("å¥–åŠ±å½’ä¸€åŒ–æ•ˆæœåˆ†æ")
    print("=" * 80)

    for scenario in ["small", "medium", "large"]:
        print(f"\n{scenario.upper()} è§„æ¨¡:")

        scenario_df = df[df["scenario"] == scenario]

        original = scenario_df[scenario_df["reward_type"] == "original"]
        scale_aware = scenario_df[scenario_df["reward_type"] == "scale_aware"]

        # æ”¹è¿›ç‡å¯¹æ¯”
        print(f"\n  æ”¹è¿›ç‡:")
        print(f"    åŸå§‹: {original['improvement'].mean():.2%} Â± {original['improvement'].std():.2%}")
        print(f"    å½’ä¸€åŒ–: {scale_aware['improvement'].mean():.2%} Â± {scale_aware['improvement'].std():.2%}")
        print(f"    æå‡: {(scale_aware['improvement'].mean() - original['improvement'].mean()):+.2%}")

        # ç»Ÿè®¡æ£€éªŒ
        _, p_value = stats.wilcoxon(original['improvement'], scale_aware['improvement'])
        print(f"    på€¼: {p_value:.4f}")

        # å¥–åŠ±æ–¹å·®å¯¹æ¯”
        print(f"\n  å¥–åŠ±æ–¹å·®:")
        print(f"    åŸå§‹: {original['reward_variance'].mean():.2f}")
        print(f"    å½’ä¸€åŒ–: {scale_aware['reward_variance'].mean():.2f}")
        variance_reduction = (1 - scale_aware['reward_variance'].mean() / original['reward_variance'].mean()) * 100
        print(f"    é™ä½: {variance_reduction:.1f}%")

    # ========== è·¨è§„æ¨¡ç¨³å®šæ€§åˆ†æ ==========
    print("\n" + "="*80)
    print("è·¨è§„æ¨¡ç¨³å®šæ€§")
    print("="*80)

    for reward_type in ["original", "scale_aware"]:
        reward_df = df[df["reward_type"] == reward_type]

        # è®¡ç®—è·¨è§„æ¨¡çš„å˜å¼‚ç³»æ•°
        improvements_by_scale = {}
        for scenario in ["small", "medium", "large"]:
            improvements_by_scale[scenario] = reward_df[reward_df["scenario"] == scenario]["improvement"].mean()

        values = list(improvements_by_scale.values())
        cross_scale_cv = np.std(values) / np.mean(values)

        print(f"\n{reward_type}:")
        print(f"  å°è§„æ¨¡: {improvements_by_scale['small']:.2%}")
        print(f"  ä¸­è§„æ¨¡: {improvements_by_scale['medium']:.2%}")
        print(f"  å¤§è§„æ¨¡: {improvements_by_scale['large']:.2%}")
        print(f"  è·¨è§„æ¨¡CV: {cross_scale_cv:.3f}")

    # ========== å¯è§†åŒ– ==========
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # å­å›¾1: æ”¹è¿›ç‡å¯¹æ¯”
    for scenario in ["small", "medium", "large"]:
        scenario_df = df[df["scenario"] == scenario]

        original_data = scenario_df[scenario_df["reward_type"] == "original"]["improvement"]
        scale_aware_data = scenario_df[scenario_df["reward_type"] == "scale_aware"]["improvement"]

        positions = [scenario, scenario]
        data = [original_data, scale_aware_data]

        axes[0].boxplot(data, positions=positions, widths=0.3)

    axes[0].set_title("Improvement Ratio Comparison")
    axes[0].set_xlabel("Scenario")
    axes[0].set_ylabel("Improvement Ratio")

    # å­å›¾2: å¥–åŠ±æ–¹å·®å¯¹æ¯”
    scenarios = ["small", "medium", "large"]
    original_vars = [df[(df["scenario"] == s) & (df["reward_type"] == "original")]["reward_variance"].mean() for s in scenarios]
    scale_aware_vars = [df[(df["scenario"] == s) & (df["reward_type"] == "scale_aware")]["reward_variance"].mean() for s in scenarios]

    x = np.arange(len(scenarios))
    width = 0.35

    axes[1].bar(x - width/2, original_vars, width, label="Original")
    axes[1].bar(x + width/2, scale_aware_vars, width, label="Scale-Aware")

    axes[1].set_title("Reward Variance Comparison")
    axes[1].set_xlabel("Scenario")
    axes[1].set_ylabel("Reward Variance")
    axes[1].set_xticks(x)
    axes[1].set_xticklabels(scenarios)
    axes[1].legend()

    plt.tight_layout()
    plt.savefig(f"{results_dir}/reward_normalization_analysis.png", dpi=300)
    print(f"\nå›¾è¡¨å·²ä¿å­˜: {results_dir}/reward_normalization_analysis.png")

if __name__ == "__main__":
    analyze_reward_normalization()
```

---

### ğŸ“Š Week 5 é¢„æœŸæˆæœ

**éªŒè¯æŒ‡æ ‡**:
1. **å¥–åŠ±æ–¹å·®é™ä½**: ç›®æ ‡ >50%ï¼ˆè·¨è§„æ¨¡ï¼‰
2. **å¤§è§„æ¨¡æ€§èƒ½æå‡**: ä»22-25% (Week 4) æå‡åˆ° 25-28%
3. **æ”¶æ•›é€Ÿåº¦**: æ›´å¿«è¾¾åˆ°ç¨³å®šQå€¼

**å¯äº¤ä»˜æˆæœ**:
- âœ… `src/config/defaults.py` æ›´æ–° (ScaleAwareRewardParams)
- âœ… `src/planner/alns.py` æ›´æ–° (_compute_scale_aware_reward)
- âœ… `results/week5/reward_normalization_analysis.png`
- âœ… `docs/experiments/week5_reward_normalization.md`

**å…³é”®å‘ç°**:
- è§„æ¨¡è‡ªé€‚åº”å¥–åŠ±æ˜¾è‘—é™ä½æ–¹å·®
- å¤§è§„æ¨¡æ€§èƒ½æ¥è¿‘æˆ–è¾¾åˆ°25%ç›®æ ‡
- Q-learningå­¦ä¹ æ›²çº¿æ›´ç¨³å®š

---

## Week 6-7: å®Œæ•´é›†æˆä¸æ¶ˆèç ”ç©¶

### ğŸ¯ Week 6ç›®æ ‡
1. åˆ›å»ºå®Œæ•´çš„ScaleAwareQLearningALNSç±»
2. æ·»åŠ è§„æ¨¡ç‰¹å®šçš„é¢„è®¾é…ç½®
3. ç«¯åˆ°ç«¯æµ‹è¯•

### ğŸ¯ Week 7ç›®æ ‡
1. è¿›è¡Œå…¨é¢æ¶ˆèç ”ç©¶
2. é‡åŒ–æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®
3. ç¡®å®šæœ€ä¼˜é…ç½®

---

### ğŸ“… Week 6, Day 1-3: åˆ›å»ºSAQLå®Œæ•´ç±»

**åˆ›å»ºæ–°æ–‡ä»¶** `src/planner/alns_saql.py`:

```python
"""Scale-Aware Q-Learning ALNS"""
from typing import Optional
from planner.alns_matheuristic import MatheuristicALNS
from planner.q_learning import ScaleAwareQLearningAgent
from planner.q_learning_init import QInitStrategy
from config.defaults import ScaleAwareRewardParams, QLearningParams
from scenario import Scenario
from solution import Solution

class ScaleAwareQLearningALNS(MatheuristicALNS):
    """ALNS with Scale-Aware Q-Learning operator selection

    é›†æˆäº†Week 1-5çš„æ‰€æœ‰æ”¹è¿›:
    - é—®é¢˜1: Q-tableåˆå§‹åŒ–ç­–ç•¥
    - é—®é¢˜2: ä¸ƒçŠ¶æ€ç©ºé—´
    - é—®é¢˜3: è§„æ¨¡è‡ªé€‚åº”epsilon
    - é—®é¢˜4: è§„æ¨¡è‡ªé€‚åº”å¥–åŠ±å½’ä¸€åŒ–
    """

    def __init__(
        self,
        scenario: Scenario,
        preset: str = "medium",
        seed: Optional[int] = None,
        *,
        init_strategy: QInitStrategy = QInitStrategy.UNIFORM,
    ):
        """
        Args:
            scenario: E-VRPåœºæ™¯
            preset: é¢„è®¾é…ç½® ("small", "medium", "large")
            seed: éšæœºç§å­
            init_strategy: Q-tableåˆå§‹åŒ–ç­–ç•¥
        """

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(scenario, preset, seed)

        # ========== ç¡®å®šé—®é¢˜è§„æ¨¡ ==========
        num_requests = len(scenario.requests)
        self.scale = self._determine_scale(num_requests)

        # ========== åˆå§‹åŒ–Scale-Aware Q-Learning Agent ==========
        self.q_agent = ScaleAwareQLearningAgent(
            destroy_operators=self.destroy_operators,
            repair_operators=self.repair_operators,
            params=self.config.q_learning,
            scale=self.scale,
            init_strategy=init_strategy,
        )

        # ========== ä½¿ç”¨è§„æ¨¡è‡ªé€‚åº”å¥–åŠ±å‚æ•° ==========
        self.sa_reward_params = ScaleAwareRewardParams()

        # ========== å¼ºåˆ¶ä½¿ç”¨Q-learningæ¨¡å¼ ==========
        self.adaptation_mode = "q_learning"

        print(f"[SAQL] åˆå§‹åŒ–å®Œæˆ:")
        print(f"  è§„æ¨¡: {self.scale}")
        print(f"  åˆå§‹epsilon: {self.q_agent.epsilon:.3f}")
        print(f"  çŠ¶æ€æ•°: {len(self.q_agent.states)}")
        print(f"  åŠ¨ä½œæ•°: {len(self.q_agent.actions)}")

    def _determine_scale(self, num_requests: int) -> str:
        """ç¡®å®šé—®é¢˜è§„æ¨¡"""
        if num_requests <= 12:
            return "small"
        elif num_requests <= 30:
            return "medium"
        else:
            return "large"

    def optimize(self) -> Solution:
        """ä¸»ä¼˜åŒ–å¾ªç¯ï¼ˆç»§æ‰¿å¹¶ä½¿ç”¨è§„æ¨¡è‡ªé€‚åº”ç»„ä»¶ï¼‰"""

        # è°ƒç”¨çˆ¶ç±»çš„optimizeï¼ˆå·²é›†æˆæ‰€æœ‰æ”¹è¿›ï¼‰
        solution = super().optimize()

        # æ‰“å°Q-learningç»Ÿè®¡
        self._print_q_stats()

        return solution

    def _print_q_stats(self):
        """æ‰“å°Q-learningç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n[SAQL] Q-Learningç»Ÿè®¡:")
        print(f"  æœ€ç»ˆepsilon: {self.q_agent.epsilon:.3f}")
        print(f"  æ¢ç´¢ç‡: {self.q_agent.exploration_count / max(self.q_agent.total_count, 1):.2%}")

        # æ‰“å°æœ€ä¼˜åŠ¨ä½œ
        print(f"\n  å„çŠ¶æ€æœ€ä¼˜åŠ¨ä½œ:")
        for state in self.q_agent.states:
            best_action = max(
                self.q_agent.q_table[state],
                key=self.q_agent.q_table[state].get
            )
            best_q = self.q_agent.q_table[state][best_action]
            print(f"    {state}: {best_action} (Q={best_q:.2f})")
```

---

### ğŸ“… Week 6, Day 4-5: æ·»åŠ é¢„è®¾é…ç½®

**ä¿®æ”¹æ–‡ä»¶** `src/config/presets.py`:

```python
"""é¢„è®¾é…ç½®"""
from config.defaults import QLearningParams

# Scale-Aware Q-Learningé¢„è®¾
SAQL_PRESETS = {
    "small": {
        "max_iterations": 1000,
        "q_learning": QLearningParams(
            initial_epsilon=0.30,      # ä»Week 2ç¡®å®š
            alpha=0.40,                # å­¦ä¹ ç‡ç•¥é«˜ï¼ˆå¿«é€Ÿæ”¶æ•›ï¼‰
            gamma=0.95,
            epsilon_decay=0.995,       # å¿«é€Ÿè¡°å‡
            epsilon_min=0.01,
        ),
        "stagnation_threshold": 80,       # ä»7çŠ¶æ€é˜ˆå€¼
        "deep_stagnation_threshold": 200,
    },

    "medium": {
        "max_iterations": 2000,
        "q_learning": QLearningParams(
            initial_epsilon=0.50,      # ä»Week 2ç¡®å®š
            alpha=0.35,
            gamma=0.95,
            epsilon_decay=0.997,       # ä¸­ç­‰è¡°å‡
            epsilon_min=0.02,
        ),
        "stagnation_threshold": 120,
        "deep_stagnation_threshold": 300,
    },

    "large": {
        "max_iterations": 4000,        # æ›´å¤šè¿­ä»£
        "q_learning": QLearningParams(
            initial_epsilon=0.70,      # ä»Week 2ç¡®å®šï¼ˆé«˜æ¢ç´¢ï¼‰
            alpha=0.30,                # å­¦ä¹ ç‡ç•¥ä½ï¼ˆç¨³å®šå­¦ä¹ ï¼‰
            gamma=0.95,
            epsilon_decay=0.998,       # æ…¢è¡°å‡
            epsilon_min=0.03,          # ä¿æŒä¸€å®šæ¢ç´¢
        ),
        "stagnation_threshold": 160,
        "deep_stagnation_threshold": 400,
    },
}

def get_saql_preset(scale: str) -> dict:
    """è·å–SAQLé¢„è®¾é…ç½®"""
    return SAQL_PRESETS.get(scale, SAQL_PRESETS["medium"])
```

---

### ğŸ“… Week 6, Day 6-7: ç«¯åˆ°ç«¯æµ‹è¯•

**åˆ›å»ºæµ‹è¯•è„šæœ¬** `scripts/week6_saql_integration_test.sh`:

```bash
#!/bin/bash
# Week 6: SAQLå®Œæ•´é›†æˆæµ‹è¯•

SEEDS=(2025 2026 2027 2028 2029 2030 2031 2032 2033 2034)
SCENARIOS=("small" "medium" "large")

for scenario in "${SCENARIOS[@]}"; do
    for seed in "${SEEDS[@]}"; do
        echo "Running SAQL on ${scenario} with seed ${seed}..."
        python scripts/run_alns_preset.py \
            --scenario ${scenario} \
            --solver saql \
            --seed ${seed} \
            --output results/week6/saql_${scenario}_seed${seed}.json
    done
done

echo "SAQL integration test complete!"
```

**æ€§èƒ½éªŒè¯è„šæœ¬** `scripts/validate_saql_performance.py`:

```python
"""éªŒè¯SAQLæ˜¯å¦è¾¾åˆ°ç›®æ ‡æ€§èƒ½"""
import json
import numpy as np
from pathlib import Path

def validate_saql_performance(results_dir: str = "results/week6"):
    """éªŒè¯SAQLæ€§èƒ½æ˜¯å¦è¾¾æ ‡"""

    # ç›®æ ‡æ€§èƒ½
    TARGETS = {
        "small": 0.60,   # â‰¥60%
        "medium": 0.40,  # â‰¥40%
        "large": 0.25,   # â‰¥25% (å…³é”®ç›®æ ‡!)
    }

    # æ”¶é›†æ•°æ®
    results = {"small": [], "medium": [], "large": []}

    for file in Path(results_dir).glob("saql_*.json"):
        parts = file.stem.split("_")
        scenario = parts[1]

        with open(file) as f:
            data = json.load(f)
            results[scenario].append(data["improvement_ratio"])

    # éªŒè¯
    print("=" * 60)
    print("SAQLæ€§èƒ½éªŒè¯")
    print("=" * 60)

    all_passed = True

    for scenario in ["small", "medium", "large"]:
        improvements = np.array(results[scenario])
        mean_improvement = improvements.mean()
        target = TARGETS[scenario]

        passed = mean_improvement >= target
        all_passed = all_passed and passed

        status = "âœ“ PASS" if passed else "âœ— FAIL"

        print(f"\n{scenario.upper()} è§„æ¨¡:")
        print(f"  ç›®æ ‡: â‰¥{target:.0%}")
        print(f"  å®é™…: {mean_improvement:.2%} Â± {improvements.std():.2%}")
        print(f"  çŠ¶æ€: {status}")

    print("\n" + "="*60)
    if all_passed:
        print("âœ“ æ‰€æœ‰ç›®æ ‡è¾¾æˆï¼å¯ä»¥è¿›å…¥æ¶ˆèç ”ç©¶é˜¶æ®µã€‚")
    else:
        print("âœ— éƒ¨åˆ†ç›®æ ‡æœªè¾¾æˆï¼Œéœ€è¦è°ƒæ•´å‚æ•°æˆ–é‡æ–°åˆ†æã€‚")

    return all_passed

if __name__ == "__main__":
    passed = validate_saql_performance()
    exit(0 if passed else 1)
```

---

### ğŸ“… Week 7: æ¶ˆèç ”ç©¶

**ç›®æ ‡**: é‡åŒ–æ¯ä¸ªç»„ä»¶å¯¹æ€§èƒ½æå‡çš„è´¡çŒ®

#### æ¶ˆèå®éªŒè®¾è®¡

æµ‹è¯•6ç§é…ç½®ï¼š

| é…ç½® | Q-Init | çŠ¶æ€ç©ºé—´ | Epsilon | å¥–åŠ±å½’ä¸€åŒ– | è¯´æ˜ |
|------|--------|---------|---------|-----------|------|
| **A (Full SAQL)** | Uniform(50) | 7-state | Adaptive | Scale-aware | å®Œæ•´ç‰ˆ |
| **B** | **Zero** | 7-state | Adaptive | Scale-aware | ç§»é™¤é—®é¢˜1ä¿®å¤ |
| **C** | Uniform(50) | **3-state** | Adaptive | Scale-aware | ç§»é™¤é—®é¢˜2ä¿®å¤ |
| **D** | Uniform(50) | 7-state | **Fixed(0.12)** | Scale-aware | ç§»é™¤é—®é¢˜3ä¿®å¤ |
| **E** | Uniform(50) | 7-state | Adaptive | **Original** | ç§»é™¤é—®é¢˜4ä¿®å¤ |
| **F (Baseline)** | Zero | 3-state | Fixed(0.12) | Original | åŸå§‹ç‰ˆæœ¬ |

#### å®éªŒè„šæœ¬

**åˆ›å»º** `scripts/week7_ablation_study.sh`:

```bash
#!/bin/bash
# Week 7: æ¶ˆèç ”ç©¶

SEEDS=(2025 2026 2027 2028 2029 2030 2031 2032 2033 2034)
SCENARIOS=("small" "medium" "large")

# é…ç½®å®šä¹‰
declare -A CONFIGS
CONFIGS[A]="uniform 7 adaptive scale_aware"    # Full SAQL
CONFIGS[B]="zero 7 adaptive scale_aware"       # æ— Q-init
CONFIGS[C]="uniform 3 adaptive scale_aware"    # æ— 7-state
CONFIGS[D]="uniform 7 fixed scale_aware"       # æ— adaptive-epsilon
CONFIGS[E]="uniform 7 adaptive original"       # æ— scale-aware-reward
CONFIGS[F]="zero 3 fixed original"             # Baseline

for config_name in A B C D E F; do
    config="${CONFIGS[$config_name]}"
    read -r init_strat state_space epsilon reward <<< "$config"

    for scenario in "${SCENARIOS[@]}"; do
        for seed in "${SEEDS[@]}"; do
            echo "Running Config ${config_name} on ${scenario} with seed ${seed}..."
            python scripts/run_alns_preset.py \
                --scenario ${scenario} \
                --solver q_learning \
                --init_strategy ${init_strat} \
                --state_space ${state_space} \
                --epsilon_config ${epsilon} \
                --reward_function ${reward} \
                --seed ${seed} \
                --output results/week7/ablation_config${config_name}_${scenario}_seed${seed}.json
        done
    done
done

echo "Ablation study complete!"
```

**è¿è¡Œé‡**: 6é…ç½® Ã— 3è§„æ¨¡ Ã— 10ç§å­ = 180æ¬¡è¿è¡Œ

#### æ¶ˆèåˆ†æè„šæœ¬

**åˆ›å»º** `scripts/analyze_ablation_study.py`:

```python
"""æ¶ˆèç ”ç©¶åˆ†æ"""
import json
import numpy as np
import pandas as pd
from pathlib import Path
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_ablation(results_dir: str = "results/week7"):
    """åˆ†ææ¶ˆèç ”ç©¶ç»“æœ"""

    # æ”¶é›†æ•°æ®
    data = []
    for file in Path(results_dir).glob("ablation_*.json"):
        parts = file.stem.split("_")
        config = parts[1].replace("config", "")  # A, B, C, D, E, F
        scenario = parts[2]

        with open(file) as f:
            result = json.load(f)

        data.append({
            "config": config,
            "scenario": scenario,
            "improvement": result["improvement_ratio"]
        })

    df = pd.DataFrame(data)

    # ========== é…ç½®æè¿° ==========
    config_descriptions = {
        "A": "Full SAQLï¼ˆå®Œæ•´ç‰ˆï¼‰",
        "B": "æ— Q-initä¼˜åŒ–",
        "C": "æ— 7çŠ¶æ€ç©ºé—´",
        "D": "æ— è‡ªé€‚åº”epsilon",
        "E": "æ— å¥–åŠ±å½’ä¸€åŒ–",
        "F": "Baselineï¼ˆåŸå§‹ç‰ˆï¼‰"
    }

    # ========== åˆ†è§„æ¨¡åˆ†æ ==========
    print("=" * 80)
    print("æ¶ˆèç ”ç©¶ï¼šç»„ä»¶è´¡çŒ®åˆ†æ")
    print("=" * 80)

    for scenario in ["small", "medium", "large"]:
        print(f"\n{'='*80}")
        print(f"{scenario.upper()} è§„æ¨¡")
        print(f"{'='*80}\n")

        scenario_df = df[df["scenario"] == scenario]

        # ç»Ÿè®¡æ‘˜è¦
        summary = scenario_df.groupby("config")["improvement"].agg([
            ("å‡å€¼", "mean"),
            ("æ ‡å‡†å·®", "std"),
            ("æœ€å°å€¼", "min"),
            ("æœ€å¤§å€¼", "max")
        ])

        # æ·»åŠ é…ç½®æè¿°
        summary.index = [f"{idx} ({config_descriptions[idx]})" for idx in summary.index]

        print(summary.to_string())

        # è®¡ç®—æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®
        print(f"\nç»„ä»¶è´¡çŒ®åˆ†æï¼ˆç›¸å¯¹äºBaseline Fï¼‰:")

        baseline_mean = scenario_df[scenario_df["config"] == "F"]["improvement"].mean()
        full_saql_mean = scenario_df[scenario_df["config"] == "A"]["improvement"].mean()

        print(f"  Baseline (F): {baseline_mean:.2%}")
        print(f"  Full SAQL (A): {full_saql_mean:.2%}")
        print(f"  æ€»æ”¹è¿›: {(full_saql_mean - baseline_mean):+.2%}\n")

        # å•ä¸ªç»„ä»¶è´¡çŒ®ï¼ˆé€šè¿‡ç§»é™¤è¯¥ç»„ä»¶çš„æ€§èƒ½ä¸‹é™æ¥ä¼°è®¡ï¼‰
        components = {
            "Q-initä¼˜åŒ–": ("A", "B"),
            "7çŠ¶æ€ç©ºé—´": ("A", "C"),
            "è‡ªé€‚åº”epsilon": ("A", "D"),
            "å¥–åŠ±å½’ä¸€åŒ–": ("A", "E")
        }

        for component_name, (with_comp, without_comp) in components.items():
            with_mean = scenario_df[scenario_df["config"] == with_comp]["improvement"].mean()
            without_mean = scenario_df[scenario_df["config"] == without_comp]["improvement"].mean()

            contribution = with_mean - without_mean
            contribution_pct = contribution / (full_saql_mean - baseline_mean) * 100 if full_saql_mean > baseline_mean else 0

            print(f"  {component_name}:")
            print(f"    è´¡çŒ®: {contribution:+.2%} ({contribution_pct:.1f}% of total)")

    # ========== äº¤äº’æ•ˆåº”åˆ†æ ==========
    print("\n" + "="*80)
    print("äº¤äº’æ•ˆåº”åˆ†æ")
    print("="*80)

    # æ£€æŸ¥ç»„ä»¶é—´æ˜¯å¦æœ‰ååŒæ•ˆåº”
    for scenario in ["small", "medium", "large"]:
        scenario_df = df[df["scenario"] == scenario]

        print(f"\n{scenario.upper()}:")

        # è®¡ç®—åŠ æ€§æ¨¡å‹çš„é¢„æœŸå€¼
        baseline = scenario_df[scenario_df["config"] == "F"]["improvement"].mean()
        full_saql = scenario_df[scenario_df["config"] == "A"]["improvement"].mean()

        # å•ç‹¬ç»„ä»¶è´¡çŒ®ä¹‹å’Œ
        contributions = []
        for _, (with_c, without_c) in components.items():
            contrib = scenario_df[scenario_df["config"] == with_c]["improvement"].mean() - \
                     scenario_df[scenario_df["config"] == without_c]["improvement"].mean()
            contributions.append(contrib)

        additive_prediction = baseline + sum(contributions)
        actual = full_saql

        synergy = actual - additive_prediction

        print(f"  åŠ æ€§æ¨¡å‹é¢„æµ‹: {additive_prediction:.2%}")
        print(f"  å®é™…æ€§èƒ½: {actual:.2%}")
        print(f"  ååŒæ•ˆåº”: {synergy:+.2%}")

    # ========== å¯è§†åŒ– ==========
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    for idx, scenario in enumerate(["small", "medium", "large"]):
        scenario_df = df[df["scenario"] == scenario]

        # æŒ‰é…ç½®åˆ†ç»„
        plot_data = []
        plot_labels = []
        for config in ["F", "B", "C", "D", "E", "A"]:
            config_data = scenario_df[scenario_df["config"] == config]["improvement"]
            plot_data.append(config_data)
            plot_labels.append(f"{config}\n{config_descriptions[config]}")

        bp = axes[idx].boxplot(plot_data, labels=plot_labels, patch_artist=True)

        # é¢œè‰²ï¼šbaselineç°è‰²ï¼Œfull SAQLç»¿è‰²ï¼Œå…¶ä»–è“è‰²
        colors = ['gray', 'lightblue', 'lightblue', 'lightblue', 'lightblue', 'lightgreen']
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)

        axes[idx].set_title(f"{scenario.upper()} Scale")
        axes[idx].set_ylabel("Improvement Ratio")
        axes[idx].tick_params(axis='x', rotation=45)
        axes[idx].grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/ablation_study_results.png", dpi=300)
    print(f"\nå›¾è¡¨å·²ä¿å­˜: {results_dir}/ablation_study_results.png")

    # ========== ä¿å­˜æ±‡æ€» ==========
    summary_data = {}
    for scenario in ["small", "medium", "large"]:
        scenario_df = df[df["scenario"] == scenario]
        summary_data[scenario] = {}
        for config in ["A", "B", "C", "D", "E", "F"]:
            config_data = scenario_df[scenario_df["config"] == config]["improvement"]
            summary_data[scenario][config] = {
                "mean": float(config_data.mean()),
                "std": float(config_data.std()),
                "description": config_descriptions[config]
            }

    with open(f"{results_dir}/ablation_summary.json", "w") as f:
        json.dump(summary_data, f, indent=2)

if __name__ == "__main__":
    analyze_ablation()
```

---

### ğŸ“Š Week 6-7 é¢„æœŸæˆæœ

**Week 6äº¤ä»˜**:
- âœ… `src/planner/alns_saql.py` (300è¡Œ)
- âœ… `src/config/presets.py` (SAQLé¢„è®¾)
- âœ… ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•é€šè¿‡
- âœ… æ€§èƒ½è¾¾æ ‡éªŒè¯é€šè¿‡

**Week 7äº¤ä»˜**:
- âœ… æ¶ˆèç ”ç©¶æ•°æ®ï¼š180æ¬¡è¿è¡Œ
- âœ… `results/week7/ablation_study_results.png`
- âœ… `results/week7/ablation_summary.json`
- âœ… `docs/experiments/week7_ablation_study.md`

**é¢„æœŸå‘ç°**:
1. **æœ€å¤§è´¡çŒ®**: 7çŠ¶æ€ç©ºé—´ï¼ˆé—®é¢˜2ï¼‰å’Œå¥–åŠ±å½’ä¸€åŒ–ï¼ˆé—®é¢˜4ï¼‰
2. **ååŒæ•ˆåº”**: å››ä¸ªç»„ä»¶ç»„åˆåæ•ˆæœ>å•ç‹¬è´¡çŒ®ä¹‹å’Œ
3. **å¤§è§„æ¨¡æ€§èƒ½**: Full SAQLè¾¾åˆ°25-28%æ”¹è¿›ç‡

**å…³é”®æŒ‡æ ‡è¾¾æˆ**:
- âœ… å¤§è§„æ¨¡æ”¹è¿›ç‡ï¼šâ‰¥25% (ç›®æ ‡è¾¾æˆï¼)
- âœ… ç§å­æ–¹å·®é™ä½ï¼š>60%
- âœ… æ‰€æœ‰ç»„ä»¶ç»éªŒè¯æœ‰æ•ˆ

---

## Week 1-7 æ€»ç»“ï¼šQ-learningé—®é¢˜ä¿®å¤å®Œæˆ

### ä¿®å¤æˆæœå¯¹æ¯”

| æŒ‡æ ‡ | ä¿®å¤å‰ (Week 0) | ä¿®å¤å (Week 7) | æ”¹è¿› |
|------|----------------|----------------|------|
| å°è§„æ¨¡æ”¹è¿›ç‡ | 62.45% | ~62% | ä¿æŒ âœ“ |
| ä¸­è§„æ¨¡æ”¹è¿›ç‡ | ~30% | ~42% | +12pp âœ“ |
| **å¤§è§„æ¨¡æ”¹è¿›ç‡** | **6.92%** | **25-28%** | **+18-21pp** âœ“âœ“âœ“ |
| å¤§è§„æ¨¡ç§å­æ–¹å·® (CV) | ~0.40 | ~0.15 | -62.5% âœ“ |

### å››ä¸ªé—®é¢˜è§£å†³æƒ…å†µ

âœ… **é—®é¢˜1**: Q-tableåˆå§‹åŒ– â†’ é‡‡ç”¨Uniform(50.0)
âœ… **é—®é¢˜2**: çŠ¶æ€ç©ºé—´ â†’ 3çŠ¶æ€ â†’ 7çŠ¶æ€
âœ… **é—®é¢˜3**: Epsilonç­–ç•¥ â†’ è§„æ¨¡è‡ªé€‚åº” (0.30/0.50/0.70)
âœ… **é—®é¢˜4**: å¥–åŠ±å½’ä¸€åŒ– â†’ è§„æ¨¡è‡ªé€‚åº”å¥–åŠ±å‡½æ•°

### ä¸‹ä¸€æ­¥ï¼ˆPhase 2ï¼‰

è¿›å…¥Week 8-13ï¼šåŠ¨æ€E-VRPåœ¨çº¿ä¼˜åŒ–
- åŠ¨æ€åœºæ™¯ç”Ÿæˆå™¨
- Anytime SAQL
- è¿ç§»å­¦ä¹ 
- å¤šä¿çœŸåº¦ä¼˜åŒ–

---

# Phase 2: åŠ¨æ€E-VRPåœ¨çº¿ä¼˜åŒ– (Week 8-13)

[å†…å®¹å°†ç»§ç»­...]

---

# æ¯å‘¨æ£€æŸ¥æ¸…å•

## Week 1 Checklist
- [ ] åŸºçº¿æ•°æ®æ”¶é›†å®Œæˆï¼ˆ30æ¬¡è¿è¡Œï¼‰
- [ ] 4ç§åˆå§‹åŒ–ç­–ç•¥å®ç°
- [ ] åˆå§‹åŒ–å®éªŒå®Œæˆï¼ˆ120æ¬¡è¿è¡Œï¼‰
- [ ] ç»Ÿè®¡åˆ†æå®Œæˆï¼Œæœ€ä¼˜ç­–ç•¥ç¡®å®š
- [ ] ä»£ç æäº¤ï¼š`q_learning_init.py`
- [ ] æ–‡æ¡£å®Œæˆï¼š`week1_q_init_analysis.md`

## Week 2 Checklist
- [ ] Epsilonå½±å“åˆ†æå®Œæˆ
- [ ] 3ç§epsiloné…ç½®å®éªŒå®Œæˆï¼ˆ90æ¬¡è¿è¡Œï¼‰
- [ ] è‡ªé€‚åº”epsilonå‡½æ•°å®ç°
- [ ] æ¨èé…ç½®ç¡®å®š
- [ ] æ–‡æ¡£å®Œæˆï¼š`week2_epsilon_analysis.md`

## Week 3-4 Checklist
- [ ] ä¸ƒçŠ¶æ€åˆ†ç±»å™¨å®ç°ï¼š`state_classifier.py`
- [ ] å•å…ƒæµ‹è¯•é€šè¿‡ï¼ˆ95%+è¦†ç›–ç‡ï¼‰
- [ ] ScaleAwareQLearningAgentå®ç°
- [ ] ALNSä¸»å¾ªç¯é›†æˆå®Œæˆ
- [ ] 3çŠ¶æ€vs7çŠ¶æ€å¯¹æ¯”å®éªŒï¼ˆ60æ¬¡è¿è¡Œï¼‰
- [ ] æ–‡æ¡£å®Œæˆï¼š`week3-4_seven_state_analysis.md`

## Week 5 Checklist
- [ ] ScaleAwareRewardParamsé…ç½®å®Œæˆ
- [ ] è§„æ¨¡è‡ªé€‚åº”å¥–åŠ±å‡½æ•°å®ç°
- [ ] A/Bæµ‹è¯•å®Œæˆï¼ˆ60æ¬¡è¿è¡Œï¼‰
- [ ] å¥–åŠ±æ–¹å·®é™ä½>50%éªŒè¯
- [ ] æ–‡æ¡£å®Œæˆï¼š`week5_reward_normalization.md`

## Week 6 Checklist
- [ ] ScaleAwareQLearningALNSç±»å®Œæˆ
- [ ] é¢„è®¾é…ç½®æ·»åŠ ï¼š`presets.py`
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡ï¼ˆ30æ¬¡è¿è¡Œï¼‰
- [ ] æ€§èƒ½ç›®æ ‡è¾¾æˆéªŒè¯

## Week 7 Checklist
- [ ] æ¶ˆèç ”ç©¶å®éªŒå®Œæˆï¼ˆ180æ¬¡è¿è¡Œï¼‰
- [ ] ç»„ä»¶è´¡çŒ®åˆ†æå®Œæˆ
- [ ] æ¶ˆèç ”ç©¶æŠ¥å‘Šï¼š`week7_ablation_study.md`
- [ ] å¤§è§„æ¨¡æ€§èƒ½â‰¥25%ç¡®è®¤

---

**æ–‡æ¡£ç‰ˆæœ¬**: 2.0
**æœ€åæ›´æ–°**: 2025-11-09
**çŠ¶æ€**: è¯¦ç»†è®¡åˆ’å·²å‡†å¤‡ï¼Œå¾…æ‰§è¡Œ
