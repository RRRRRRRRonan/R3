# Week 2 æµ‹è¯•æ–¹æ¡ˆï¼šè‡ªé€‚åº”Epsilonç­–ç•¥å®éªŒ

**æ—¥æœŸ**: 2025-11-12
**çŠ¶æ€**: ğŸ”„ è®¾è®¡ä¸­
**ç›®æ ‡**: æµ‹è¯•è‡ªé€‚åº”exploration rateæ˜¯å¦èƒ½æ”¹å–„å¤§è§„æ¨¡Q-learningæ€§èƒ½
**é¢„è®¡æ—¶é—´**: 2-3å¤©
**é¢„è®¡è¿è¡Œæ¬¡æ•°**: 90æ¬¡ (3ç­–ç•¥ Ã— 3è§„æ¨¡ Ã— 10ç§å­)

---

## ğŸ“‹ å®éªŒåŠ¨æœº

### Week 1 çš„å‘ç°

Week 1å®éªŒè¡¨æ˜Q-tableåˆå§‹åŒ–å¯¹æ€§èƒ½å½±å“å¾ˆå°ï¼Œä½†æå‡ºäº†ä¸€ä¸ªé‡è¦å‡è®¾ï¼š

**å½“å‰epsilonç­–ç•¥é—®é¢˜**:
- å›ºå®šåˆå§‹å€¼ï¼š0.12ï¼ˆæ‰€æœ‰è§„æ¨¡ï¼‰
- è¿™å¯¹å¤§è§„æ¨¡é—®é¢˜å¯èƒ½ä¸å¤Ÿæ¢ç´¢

**è¯æ®**:
1. å¤§è§„æ¨¡æ€§èƒ½è¿œä½äºä¸­å°è§„æ¨¡ï¼ˆ25% vs 37%ï¼‰
2. Q-learningåœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹æ–¹å·®æ›´é«˜ï¼ˆCV=0.595ï¼‰
3. Matheuristicï¼ˆæ— Q-learningï¼‰åœ¨å¤§è§„æ¨¡è¡¨ç°æ›´å¥½ï¼ˆ27% vs 7%ï¼‰

**å‡è®¾**: å¤§è§„æ¨¡é—®é¢˜çš„è§£ç©ºé—´æ›´å¤§ï¼Œéœ€è¦æ›´å¤šæ¢ç´¢æ‰èƒ½æ‰¾åˆ°å¥½è§£ã€‚

---

## ğŸ¯ ç ”ç©¶é—®é¢˜

**Primary Question**: æé«˜å¤§è§„æ¨¡é—®é¢˜çš„exploration rateæ˜¯å¦èƒ½æ˜¾è‘—æ”¹å–„Q-learningæ€§èƒ½ï¼Ÿ

**Hypotheses**:
- H1: Scale-adaptive epsilonï¼ˆæ ¹æ®è§„æ¨¡è°ƒæ•´ï¼‰å°†æ”¹å–„å¤§è§„æ¨¡æ€§èƒ½
- H2: ç»Ÿä¸€æé«˜epsilonè‡³0.50å°†æ”¹å–„æ‰€æœ‰è§„æ¨¡çš„æ€§èƒ½
- H3: å½“å‰epsilonï¼ˆ0.12ï¼‰å¯¹å°è§„æ¨¡è¶³å¤Ÿï¼Œä½†å¯¹å¤§è§„æ¨¡ä¸è¶³

---

## ğŸ”¬ å®éªŒè®¾è®¡

### Epsilonç­–ç•¥

#### Strategy 1: CURRENT (Baseline)
```python
initial_epsilon = 0.12  # All scales
epsilon_decay = 0.995
min_epsilon = 0.01
```
**Rationale**: Current implementation, baseline for comparison

#### Strategy 2: SCALE_ADAPTIVE
```python
# Scale-dependent initial epsilon
scale_epsilon_map = {
    "small": 0.30,   # 2.5Ã— current
    "medium": 0.50,  # 4.2Ã— current
    "large": 0.70    # 5.8Ã— current
}
epsilon_decay = 0.995
min_epsilon = 0.05  # Higher floor for sustained exploration
```
**Rationale**: Larger problems need more exploration due to exponentially larger search spaces

**Justification**:
- Small (8 requests): ~8! Ã— 5^8 â‰ˆ 10^9 possible solutions
- Medium (24 requests): ~24! Ã— 15^24 â‰ˆ 10^30 possible solutions
- Large (40 requests): ~40! Ã— 25^40 â‰ˆ 10^60 possible solutions

#### Strategy 3: HIGH_UNIFORM
```python
initial_epsilon = 0.50  # All scales
epsilon_decay = 0.995
min_epsilon = 0.05
```
**Rationale**: Test if high exploration universally beneficial (simpler than scale-adaptive)

---

## ğŸ“Š Experimental Protocol

### Problem Configuration

**Scenarios** (Same as Week 1):
- Small: 8 requests, 5 stations (seed 11)
- Medium: 24 requests, 15 stations (seed 19)
- Large: 40 requests, 25 stations (seed 17)

**Optimization Seeds**: 10 per configuration (2025-2034)

**Fixed Parameters** (Consistent with Week 1):
- Iterations: 40 (small/medium), 44 (large)
- Alpha (learning rate): 0.40
- Gamma (discount): 0.95
- Q-table initialization: ZERO (based on Week 1 recommendation)

**Experiment Matrix**:
```
3 epsilon strategies Ã— 3 scales Ã— 10 seeds = 90 experiments
```

### Execution Plan

**Day 1**: Implementation
- Modify `src/planner/alns_matheuristic.py` to support scale-adaptive epsilon
- Add `epsilon_strategy` parameter to ALNS initialization
- Create `scripts/week2/run_experiment.py` (adapted from week1)

**Day 2**: Execution
- Run experiments in parallel (3 processes: small/medium/large)
- Estimated time: 6-8 hours

**Day 3**: Analysis
- Statistical comparison vs. CURRENT baseline
- Convergence curve analysis (epsilon decay trajectory)
- Decision: Continue with best strategy or revert to CURRENT

---

## ğŸ“ˆ Evaluation Metrics

### Primary Metrics

1. **Improvement Ratio** (main metric)
   ```
   improvement_ratio = (baseline_cost - optimised_cost) / baseline_cost
   ```

2. **Statistical Significance**
   - Wilcoxon signed-rank test vs. CURRENT baseline
   - Significance levels: * p<0.10, ** p<0.05, *** p<0.01
   - Cohen's d effect size (|d| > 0.3 considered meaningful)

### Secondary Metrics

3. **Convergence Analysis**
   - Q-value stabilization iteration
   - Epsilon trajectory (initial â†’ final)
   - Exploration vs. exploitation ratio

4. **Variance Analysis**
   - Standard deviation across seeds
   - Coefficient of variation (CV)
   - Compare to Week 1 ZERO baseline

5. **Runtime**
   - Total runtime per experiment
   - Iterations per second

---

## ğŸ¯ Success Criteria

### Minimal Success
- **Large scale improvement**: +5% over CURRENT (25% â†’ 30%)
- **Statistical significance**: p < 0.05, |Cohen's d| > 0.3
- **No degradation on small/medium**: Within 2% of CURRENT

### Full Success
- **Large scale improvement**: +8% or more (25% â†’ 33%+)
- **Medium scale improvement**: +3% or more (31% â†’ 34%+)
- **Statistical significance**: p < 0.01, |Cohen's d| > 0.5
- **Reduced variance**: CV reduction by 20%+

### Expected Outcome
Based on literature and theory, we expect:
- SCALE_ADAPTIVE to improve large-scale performance by 3-7%
- HIGH_UNIFORM to improve large-scale but may hurt small-scale
- Trade-off: More exploration â†’ Better solutions but higher variance

---

## ğŸ”§ Implementation Details

### Code Changes

**New File**: `src/planner/epsilon_strategy.py`
```python
@dataclass
class EpsilonStrategy:
    """Defines epsilon (exploration rate) strategy."""
    name: str
    initial_epsilon: float
    decay_rate: float = 0.995
    min_epsilon: float = 0.01

    @staticmethod
    def current() -> "EpsilonStrategy":
        """Current baseline strategy."""
        return EpsilonStrategy("current", 0.12, 0.995, 0.01)

    @staticmethod
    def scale_adaptive(num_requests: int) -> "EpsilonStrategy":
        """Scale-adaptive strategy."""
        if num_requests <= 12:
            initial = 0.30
        elif num_requests <= 30:
            initial = 0.50
        else:
            initial = 0.70
        return EpsilonStrategy("scale_adaptive", initial, 0.995, 0.05)

    @staticmethod
    def high_uniform() -> "EpsilonStrategy":
        """High uniform exploration strategy."""
        return EpsilonStrategy("high_uniform", 0.50, 0.995, 0.05)
```

**Modified File**: `src/planner/alns_matheuristic.py`
- Add `epsilon_strategy` parameter to `__init__`
- Pass strategy to Q-learning agent initialization

**Modified File**: `src/planner/q_learning.py`
- Accept epsilon parameters from strategy
- Update initialization to use strategy values

### Experiment Script

**Location**: `scripts/week2/run_experiment.py`

**Usage**:
```powershell
python scripts\week2\run_experiment.py \
    --scenario small \
    --epsilon_strategy scale_adaptive \
    --seed 2025 \
    --output results\week2\epsilon_scale_adaptive_small_seed2025.json
```

**Batch Scripts**:
- `scripts/week2/01_current_baseline_*.bat` (baseline, should match Week 1 ZERO)
- `scripts/week2/02_scale_adaptive_*.bat`
- `scripts/week2/03_high_uniform_*.bat`

---

## ğŸ“Š Analysis Plan

### Statistical Analysis Script

**Location**: `scripts/week2/analyze_epsilon.py`

**Features**:
1. Load all 90 experiment results
2. Compute descriptive statistics per strategy/scale
3. Wilcoxon tests (each strategy vs. CURRENT)
4. Cohen's d effect sizes
5. Convergence curve plotting (epsilon over iterations)
6. Generate summary report

**Output**:
- `results/week2/analysis_summary.txt`
- `results/week2/convergence_curves.png`
- `results/week2/improvement_comparison.png`

### Decision Tree

```
If SCALE_ADAPTIVE improves large by â‰¥5% (p<0.05):
  âœ… Adopt SCALE_ADAPTIVE for all future experiments
  âœ… Proceed to Week 5 (Reward Normalization)

Else if HIGH_UNIFORM improves large by â‰¥5% (p<0.05):
  âš ï¸ Consider trade-offs (may hurt small-scale)
  âš ï¸ May need scale-specific configurations

Else (no improvement):
  âŒ Epsilon is not the bottleneck
  âŒ Skip directly to Week 5 (Reward Normalization)
  âŒ Hypothesis: Reward signal quality matters more than exploration
```

---

## ğŸ”— Relation to Week 1

**What Week 1 Told Us**:
- Q-table initialization has minimal impact (|d| < 0.2)
- ACTION_SPECIFIC with domain bias was harmful
- ZERO baseline is most robust

**What Week 2 Tests**:
- Whether exploration rate (not initialization) is the key factor
- Whether scale-dependent behavior requires scale-dependent parameters
- Quick test (2-3 days) before committing to Week 5 (5-7 days)

**Combined Insight** (if Week 2 succeeds):
- Initialization: Use ZERO (Week 1)
- Exploration: Use SCALE_ADAPTIVE epsilon (Week 2)
- Next: Test if reward normalization adds further benefit (Week 5)

---

## ğŸ“š Literature Support

**Sutton & Barto (2018)**: "Reinforcement Learning: An Introduction"
- Chapter 2.7: Optimistic initial values encourage exploration
- Chapter 6.5: Importance of exploration in large state spaces
- Finding: Exploration matters more when value estimates are uncertain

**Silva et al. (2019)**: "Q-learning for ALNS"
- Used fixed epsilon=0.10, did not test scale-adaptive strategies
- Noted performance degradation on large instances (but did not address)

**Auer et al. (2002)**: "UCB for multi-armed bandits"
- Theoretical justification for exploration in large action spaces
- Suggests exploration rate should scale with log(action_space_size)

**Contribution**: First systematic study of scale-adaptive exploration for Q-learning in metaheuristics.

---

## ğŸš¨ Risk Mitigation

### Risk 1: Higher Epsilon Increases Variance
**Probability**: Medium
**Impact**: Medium
**Mitigation**:
- 10 seeds provide statistical power to detect true signal
- Wilcoxon test robust to outliers
- If variance too high, can adjust decay rate (faster decay)

### Risk 2: No Improvement Despite Higher Exploration
**Probability**: Medium
**Impact**: Low (quick experiment, low cost)
**Mitigation**:
- Confirms that exploration is not the bottleneck
- Pivots focus to Week 5 (reward normalization)
- Not a wasted effortâ€”negative results are informative

### Risk 3: Trade-off Between Small and Large Scale
**Probability**: Low
**Impact**: Medium
**Mitigation**:
- SCALE_ADAPTIVE avoids this by using different epsilon per scale
- If trade-off exists, document and justify scale-specific configs

---

## ğŸ“… Timeline

| Day | Task | Hours | Deliverable |
|-----|------|-------|-------------|
| 1 | Design & Implementation | 4-6h | Code ready, scripts written |
| 2 | Run experiments | 6-8h | 90 result files |
| 3 | Analysis & Report | 3-4h | Summary document, decision |

**Total**: 13-18 hours over 2-3 days

---

## âœ… Checklist

### Pre-Experiment
- [ ] Create `src/planner/epsilon_strategy.py`
- [ ] Modify `src/planner/q_learning.py` to accept strategy
- [ ] Modify `src/planner/alns_matheuristic.py` to pass strategy
- [ ] Create `scripts/week2/run_experiment.py`
- [ ] Create batch scripts (Ã—9: 3 strategies Ã— 3 scales)
- [ ] Test single run to verify correctness

### Execution
- [ ] Run CURRENT baseline (30 experiments)
- [ ] Run SCALE_ADAPTIVE (30 experiments)
- [ ] Run HIGH_UNIFORM (30 experiments)
- [ ] Verify all 90 result files generated
- [ ] Spot-check epsilon values in output JSON

### Analysis
- [ ] Run `analyze_epsilon.py`
- [ ] Generate convergence curves
- [ ] Statistical tests (Wilcoxon, Cohen's d)
- [ ] Write WEEK2_RESULTS.md (similar to Week 1)
- [ ] Make decision: adopt strategy or proceed to Week 5

### Documentation
- [ ] Update `SAQL_IMPLEMENTATION_PLAN_2025-11-09.md` with Week 2 results
- [ ] Commit and push all code/results
- [ ] Update Option A timeline based on findings

---

## ğŸ“– References

1. Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). MIT Press.

2. Silva, T. L. C., de Queiroz, T. A., & Munari, P. (2019). An improved adaptive large neighborhood search algorithm for the resource constrained project scheduling problem. Engineering Optimization, 51(12), 2063-2088.

3. Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3), 235-256.

4. Ropke, S., & Pisinger, D. (2006). An adaptive large neighborhood search heuristic for the pickup and delivery problem with time windows. Transportation Science, 40(4), 455-472.

---

## Document Metadata

**Version**: 1.0
**Created**: 2025-11-12
**Status**: Design Complete, Ready for Implementation
**Dependencies**: Week 1 results (ZERO initialization recommendation)
**Next**: Week 5 (Reward Normalization) or Week 3-4 (7-State MDP, conditional)

---

**End of Document**
